Metadata-Version: 2.4
Name: workzone
Version: 1.0.0
Summary: AI-powered construction zone detection and monitoring system for ESV competition
Author-email: WMaia9 <contact@workzone.dev>
License: MIT
Project-URL: Homepage, https://github.com/WMaia9/workzone
Project-URL: Documentation, https://github.com/WMaia9/workzone#readme
Project-URL: Repository, https://github.com/WMaia9/workzone.git
Project-URL: Issues, https://github.com/WMaia9/workzone/issues
Keywords: yolo,construction,detection,computer-vision,esv
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: ultralytics>=8.0.0
Requires-Dist: opencv-python>=4.8.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: torch>=2.0.0
Requires-Dist: torchvision>=0.15.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: tqdm>=4.65.0
Requires-Dist: Pillow>=10.0.0
Requires-Dist: wandb>=0.15.0
Requires-Dist: streamlit>=1.28.0
Requires-Dist: scikit-learn>=1.3.0
Requires-Dist: matplotlib>=3.7.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: scipy>=1.10.0
Requires-Dist: open_clip_torch>=2.20.0
Requires-Dist: easyocr>=1.7.0
Requires-Dist: plotly>=5.15.0
Requires-Dist: seaborn>=0.12.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.7.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.4.0; extra == "dev"
Requires-Dist: pre-commit>=3.3.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=7.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.3.0; extra == "docs"

# ğŸš§ WorkZone Detection System

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![YOLOv12](https://img.shields.io/badge/YOLO-v12-00FFFF.svg)](https://github.com/ultralytics/ultralytics)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Real-time construction work zone detection and monitoring system using state-of-the-art computer vision.**

Built for ESV (Enhanced Safety of Vehicles) competition. Features multi-modal verification (YOLO + CLIP + OCR), temporal attention, scene context classification, and edge deployment optimization for NVIDIA Jetson Orin.

---

## âš¡ Key Features

| Feature | Description |
|---------|-------------|
| **ğŸ¯ YOLO12s Detection** | 50-class object detection with 84.6% false positive reduction |
| **ğŸ§  Multi-Modal Fusion** | CLIP semantic verification + OCR text extraction |
| **ï¿½ OCR Speed Sign Detection** | Priority-based OCR with visual alerts for speed limit signs |
| **ğŸ“Š Per-Cue Verification** | Per-cue confidence tracking + motion plausibility |
| **ğŸŒ Scene Context** | Highway/Urban/Suburban classification (92.8% accuracy) |
| **ğŸ”„ Adaptive State Machine** | Context-aware thresholds: OUT â†’ APPROACHING â†’ INSIDE â†’ EXITING |
| **âš¡ Edge Optimized** | Runs 30 FPS @ 1280px on Jetson Orin |
| **ğŸ¬ Interactive UI** | Streamlit calibration app with video player and real-time tuning |

---

## ğŸ¯ Performance Highlights

| Component | Metric | Value |
|-----------|--------|-------|
| **YOLO Detection** | False Positive Reduction | **84.6%** vs baseline |
| **YOLO Detection** | Inference Speed (A100) | **~85 FPS** @ 1280px |
| **YOLO Detection** | Inference Speed (Jetson) | **~30 FPS** @ 1280px |
| **Scene Context** | Classification Accuracy | **92.8%** |
| **OCR Classification** | Test Accuracy | **97.7%** (43/44) |
| **System** | GPU Memory (batch=1) | **2.4 GB** |

---

## ğŸ“¦ Installation

### Prerequisites

- **Python**: 3.10 or 3.11 (3.12 not tested)
- **OS**: Linux (Ubuntu 20.04+), macOS, Windows
- **GPU**: NVIDIA GPU with CUDA 11.8+ (recommended) or CPU
- **RAM**: 16GB minimum, 32GB recommended
- **Disk**: ~10GB for models + data

### Step 1: Clone Repository

```bash
git clone https://github.com/WMaia9/workzone.git
cd workzone
```

### Step 2: Create Virtual Environment

```bash
# Create venv
python -m venv venv

# Activate (Linux/macOS)
source venv/bin/activate

# Activate (Windows)
venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
# Install core dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Install workzone package
pip install -e .
```

**Note**: For **CPU-only** installation, install PyTorch CPU version first:
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
pip install -e .
```

### Step 4: Verify Installation

```bash
# Verify all dependencies are correctly installed
scripts/verify_installation.sh
```

This script checks:
- âœ… Python virtual environment
- âœ… All required Python packages
- âœ… WorkZone package installation
- âœ… Model weights availability

### Step 5: Quick Test

```bash
# Quick test - process demo video
python scripts/process_video_fusion.py \
  data/demo/boston_workzone_short.mp4 \
  --output-dir outputs/test \
  --stride 5

# Expected: Annotated video + CSV timeline in outputs/test/
```

### Troubleshooting

**Common Issues:**

- **"ModuleNotFoundError: No module named 'workzone'"**
  ```bash
  # Install the package
  pip install -e .
  ```

- **CUDA/PyTorch issues**
  ```bash
  # For CPU-only installation
  pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
  ```

- **Permission issues**
  ```bash
  # Make scripts executable
  chmod +x scripts/setup.sh scripts/launch_streamlit.sh scripts/verify_installation.sh
  ```

- **Model loading issues**
  ```bash
  # Check model files
  ls -la weights/
  ```

---

## ğŸš€ Quick Start

### Run the App (Professional Way)

```bash
# Quick launch
make app
# or
make streamlit
```

The app will be available at `http://localhost:8502`

### Option 1: Interactive Calibration App (Recommended)

The **Streamlit calibration UI** provides interactive parameter tuning:

**Alternative manual launch**:
```bash
source venv/bin/activate
streamlit run src/workzone/apps/streamlit/app_phase2_1_evaluation.py
# or
scripts/launch_streamlit.sh
```

**Features**:
- ğŸ“¹ Real-time video preview with live parameter adjustment
- ğŸ“Š Batch processing with comprehensive explainability dashboards
- ğŸ¬ Built-in video player with play/pause/seek controls (batch mode)
- ğŸ’¾ Export annotated videos (H.264) + detailed CSV timelines
- ğŸ›ï¸ Calibrate YOLO weights, CLIP fusion, OCR boost, state machine thresholds
- ğŸ“ OCR customization: confidence threshold, score boost, speed sign priority
- ğŸ”¬ Per-cue confidences + motion plausibility tracking
- âš¡ Component throughput visualization (Hz, ms/frame)
- ğŸ“ˆ Advanced analysis: score zones, state distribution, latency profiling

ğŸ‘‰ **See [APP_TESTING_GUIDE.md](docs/guides/APP_TESTING_GUIDE.md)** for detailed usage instructions.

#### Backend Selection (Auto/TensorRT/GPU/CPU)

In the Streamlit sidebar, choose your inference backend under "Model + Device":

- Auto (prefer TensorRT): uses `.engine` if present; else CUDA; else CPU
- TensorRT: forces `.engine` (Tensor Cores); falls back to `.pt` if load fails
- GPU (cuda): forces `.pt` on CUDA even if `.engine` exists (useful for comparisons)
- CPU: forces `.pt` on CPU for portability

Expected logs:

```
ğŸš€ TensorRT engine found: yolo12s_hardneg_1280.engine
âœ“ Loaded TensorRT model (optimized for Tensor Cores)

Loaded YOLO from yolo12s_hardneg_1280.pt (FP16/FP32)
```

#### Convert YOLO to TensorRT (optional, for maximum speed)

```bash
source venv/bin/activate
# Convert a specific model to .engine (FP16)
python scripts/optimize_for_jetson.py --model weights/yolo12s_hardneg_1280.pt

# Or convert all .pt models in weights/
python scripts/optimize_for_jetson.py
```

After conversion, run Streamlit (Auto/TensorRT will pick the `.engine` automatically).

For Jetson-specific tips and deployment steps, see [JETSON_OPTIMIZATION.md](docs/JETSON_OPTIMIZATION.md).

---

### Option 2: Command-Line Batch Processing

Process videos from the command line for high-throughput workflows:

#### Basic Usage

```bash
python scripts/process_video_fusion.py \
  path/to/video.mp4 \
  --output-dir outputs/my_run
```

#### Multi-Cue Temporal Detection

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/multi_cue \
  --enable-phase1-1 \
  --no-motion
```

#### Scene Context Classification

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/scene_context \
  --enable-phase1-4 \
  --enable-ocr
```

#### Per-Cue Verification + Motion Tracking

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/per_cue_motion \
  --enable-phase2-1 \
  --enable-phase1-1 \
  --enable-ocr \
  --no-motion \
  --stride 2
```

#### Full Pipeline (All Features)

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/full \
  --enable-phase1-1 \
  --enable-phase1-4 \
  --enable-phase2-1 \
  --enable-ocr \
  --device cuda \
  --stride 2 \
  --clip-weight 0.35 \
  --clip-trigger-th 0.45 \
  --enter-th 0.70 \
  --exit-th 0.45
```

#### Common Options

| Option | Description | Default |
|--------|-------------|---------|
| `--device` | Device: `cuda` or `cpu` | `cuda` |
| `--conf` | YOLO confidence threshold | `0.25` |
| `--stride` | Frame stride (1 = every frame) | `2` |
| `--enable-ocr` | Enable OCR text extraction | `False` |
| `--enable-phase1-1` | Multi-cue temporal logic | `False` |
| `--enable-phase1-4` | Scene context classification | `False` |
| `--enable-phase2-1` | Per-cue CLIP + motion tracking | `False` |

**Note**: Flag names preserve `phase` for backward compatibility but represent unified system components.
| `--clip-weight` | CLIP fusion weight | `0.35` |
| `--clip-trigger-th` | CLIP trigger threshold | `0.45` |
| `--enter-th` | WORKZONE entry threshold | `0.70` |
| `--exit-th` | WORKZONE exit threshold | `0.45` |
| `--no-video` | Skip video output (faster) | `False` |
| `--no-csv` | Skip CSV output | `False` |

---

## ğŸ“Š System Architecture

### Detection Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Video    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLO12s Object Detection                       â”‚
â”‚  â€¢ 50 work zone classes                         â”‚
â”‚  â€¢ Hard-negative trained (84.6% FP reduction)   â”‚
â”‚  â€¢ 1280px @ 30 FPS (Jetson Orin)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                  â–¼                  â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLIP Semantic  â”‚  â”‚ OCR Text        â”‚  â”‚ Scene Contextâ”‚  â”‚ Per-Cue CLIP    â”‚
â”‚ Verification   â”‚  â”‚ Extraction      â”‚  â”‚ Classifier   â”‚  â”‚ Verification    â”‚
â”‚ (Global)       â”‚  â”‚ (Message Boards)â”‚  â”‚ (Highway/    â”‚  â”‚ â€¢ Channelizationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  Urban/      â”‚  â”‚ â€¢ Workers       â”‚
         â”‚                   â”‚           â”‚  Suburban)   â”‚  â”‚ â€¢ Vehicles      â”‚
         â”‚                   â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ Signs         â”‚
         â”‚                   â”‚                  â”‚          â”‚ â€¢ Equipment     â”‚
         â”‚                   â”‚                  â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                  â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Multi-Modal Fusion   â”‚
                          â”‚  â€¢ Weighted EMA       â”‚
                          â”‚  â€¢ Context Boost      â”‚
                          â”‚  â€¢ OCR Boost          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  State Machine        â”‚
                          â”‚  OUT â†’ APPROACHING    â”‚
                          â”‚      â†’ INSIDE         â”‚
                          â”‚      â†’ EXITING â†’ OUT  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Output               â”‚
                          â”‚  â€¢ Annotated Video    â”‚
                          â”‚  â€¢ Timeline CSV       â”‚
                          â”‚  â€¢ State Transitions  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase Progression

| Phase | Feature | Description |
|-------|---------|-------------|
| **1.0** | Base System | YOLO + CLIP + EMA + State Machine |
| **1.1** | Multi-Cue Logic | Temporal persistence tracking (5 cue types) |
| **1.2** | Hard-Negative Mining | 84.6% FP reduction through iterative training |
| **1.3** | Motion Validation | Trajectory-based false positive filtering |
| **1.4** | Scene Context | Highway/Urban/Suburban classification (92.8%) |
| **2.1** | Per-Cue Verification | CLIP confidence per cue + motion plausibility |

---

## ğŸ“‚ Repository Structure

```
workzone/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ APP_TESTING_GUIDE.md              # Comprehensive calibration guide
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pyproject.toml                    # Package configuration
â”œâ”€â”€ setup.py                          # Installation script
â”‚
â”œâ”€â”€ configs/                          # Configuration files
â”‚   â”œâ”€â”€ config.yaml                   # Main config
â”‚   â”œâ”€â”€ multi_cue_config.yaml         # Phase 1.1 multi-cue settings
â”‚   â””â”€â”€ motion_cue_config.yaml        # Phase 1.3 motion settings
â”‚
â”œâ”€â”€ data/                             # Data directory (gitignored)
â”‚   â”œâ”€â”€ 01_raw/                       # Raw videos
â”‚   â”œâ”€â”€ 02_processed/                 # Processed annotations
â”‚   â”œâ”€â”€ 03_demo/                      # Demo videos
â”‚   â”œâ”€â”€ 04_derivatives/               # Hard-negative mining outputs
â”‚   â””â”€â”€ 05_workzone_yolo/             # YOLO training data
â”‚
â”œâ”€â”€ weights/                          # Pre-trained models (download via script)
â”‚   â”œâ”€â”€ yolo12s_hardneg_1280.pt      # Recommended model
â”‚   â”œâ”€â”€ yolo12s_fusion_baseline.pt   # Baseline model
â”‚   â”œâ”€â”€ scene_context_classifier.pt  # Phase 1.4 model
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                          # CLI tools
â”‚   â”œâ”€â”€ process_video_fusion.py       # Main video processing CLI
â”‚   â”œâ”€â”€ download_models.sh            # Model download script
â”‚   â”œâ”€â”€ mine_hard_negatives.py        # Hard-negative mining
â”‚   â”œâ”€â”€ train_scene_context.py        # Scene context training
â”‚   â””â”€â”€ evaluate_phase1_4.py          # Phase 1.4 evaluation
â”‚
â”œâ”€â”€ src/workzone/                     # Core package
â”‚   â”œâ”€â”€ detection/                    # Detection components
â”‚   â”‚   â”œâ”€â”€ yolo_detector.py          # YOLO wrapper
â”‚   â”‚   â””â”€â”€ cue_classifier.py         # Multi-cue classification
â”‚   â”œâ”€â”€ fusion/                       # Multi-modal fusion
â”‚   â”‚   â”œâ”€â”€ clip_verifier.py          # CLIP semantic verification
â”‚   â”‚   â””â”€â”€ multi_cue_gate.py         # Phase 1.1 AND gate
â”‚   â”œâ”€â”€ ocr/                          # OCR text extraction
â”‚   â”‚   â”œâ”€â”€ text_detector.py          # EasyOCR/Paddle wrapper
â”‚   â”‚   â””â”€â”€ text_classifier.py        # Text category classification
â”‚   â”œâ”€â”€ models/                       # Advanced models
â”‚   â”‚   â”œâ”€â”€ scene_context.py          # Phase 1.4 scene classifier
â”‚   â”‚   â”œâ”€â”€ per_cue_verification.py   # Phase 2.1 per-cue CLIP
â”‚   â”‚   â””â”€â”€ trajectory_tracking.py    # Phase 2.1 motion plausibility
â”‚   â”œâ”€â”€ temporal/                     # Temporal logic
â”‚   â”‚   â””â”€â”€ persistence_tracker.py    # Phase 1.1 persistence
â”‚   â”œâ”€â”€ state/                        # State machine
â”‚   â”‚   â””â”€â”€ workzone_states.py        # State transitions
â”‚   â””â”€â”€ apps/                         # Applications
â”‚       â””â”€â”€ streamlit/                # Streamlit UI
â”‚           â””â”€â”€ app_phase2_1_evaluation.py  # Calibration app
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_workzone_yolo_setup.ipynb
â”‚   â”œâ”€â”€ 02_workzone_yolo_train_eval.ipynb
â”‚   â”œâ”€â”€ 03_workzone_yolo_video_demo.ipynb
â”‚   â”œâ”€â”€ 04_workzone_video_state_machine.ipynb
â”‚   â”œâ”€â”€ 05_workzone_video_timeline_calibration.ipynb
â”‚   â”œâ”€â”€ 06_triggered_vlm_semantic_verification.ipynb
â”‚   â””â”€â”€ 07_phase1_4_scene_context.ipynb
â”‚
â”œâ”€â”€ tests/                            # Unit tests
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_pipelines.py
â”‚
â”œâ”€â”€ docs/                             # Documentation
â”‚   â”œâ”€â”€ MODEL_REGISTRY.md             # Model performance metrics
â”‚   â”œâ”€â”€ PHASE1_3.md                   # Phase 1.3 motion validation
â”‚   â””â”€â”€ guides/                       # User guides
â”‚
â””â”€â”€ outputs/                          # Processing outputs (gitignored)
    â”œâ”€â”€ phase1_1_demo/
    â”œâ”€â”€ phase1_4_demo/
    â”œâ”€â”€ phase2_1_demo/
    â””â”€â”€ ...
```

---

## ğŸ”¬ Advanced Usage

### Training Custom Models

#### YOLO Fine-tuning

```bash
# Fine-tune YOLO12 on workzone dataset
python -m workzone.cli.train_yolo \
  --model yolo12s.pt \
  --data data/05_workzone_yolo/workzone_yolo.yaml \
  --imgsz 1280 \
  --batch 32 \
  --epochs 50 \
  --device 0 \
  --run-name yolo12_fine_tuning \
  --project workzone_yolo_fine_tuning
```

#### YOLO11 Training (Latest Architecture)

```bash
# Train with YOLO11 at 1080px resolution using 2 GPUs
python -m workzone.cli.train_yolo \
  --model yolo11.yaml \
  --data data/05_workzone_yolo/workzone_yolo.yaml \
  --imgsz 1080 \
  --batch 96 \
  --epochs 100 \
  --device 0,1 \
  --run-name yolo11_1080px_training \
  --project workzone_yolo11_training
```

#### Scene Context Training

```bash
python scripts/train_scene_context.py \
  --data-root data/05_workzone_yolo \
  --output-dir runs/scene_context \
  --epochs 30 \
  --batch-size 32 \
  --backbone resnet18
```

#### Phase 2.1 Temporal Attention Training

```bash
python scripts/train_phase2_1_attention.py \
  --data-path data/phase2_1_trajectories \
  --output-dir runs/phase2_1_attention \
  --epochs 50 \
  --batch-size 16 \
  --device cuda
```

### Hard-Negative Mining

See [docs/reports/PHASE1_2_MINING_REPORT.md](docs/reports/PHASE1_2_MINING_REPORT.md) for details.

```bash
# 1. Mine candidates from video dataset
bash scripts/HARDNEG_QUICKSTART.sh

# 2. Review and categorize
python scripts/review_hard_negatives.py

# 3. Consolidate annotations
python scripts/consolidate_candidates.py

# 4. Retrain YOLO
python -m workzone.cli.train_yolo \
  --model yolo12s.pt \
  --data data/05_workzone_yolo/workzone_yolo.yaml \
  --imgsz 1280 \
  --device 0,1 \
  --batch 32 \
  --epochs 100 \
  --run-name yolo12_hardneg_training \
  --project workzone_yolo_training
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=workzone --cov-report=html

# Run specific test
pytest tests/test_models.py -v
```

---

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| [docs/guides/APP_TESTING_GUIDE.md](docs/guides/APP_TESTING_GUIDE.md) | **Comprehensive calibration guide** with all parameters explained |
| [docs/JETSON_OPTIMIZATION.md](docs/JETSON_OPTIMIZATION.md) | Edge deployment for Jetson Orin |
| [docs/technical/MODEL_REGISTRY.md](docs/technical/MODEL_REGISTRY.md) | Model performance benchmarks |
| [docs/technical/STREAMLIT_COMPONENT_ANALYSIS.md](docs/technical/STREAMLIT_COMPONENT_ANALYSIS.md) | Component status & architecture |
| [docs/REPOSITORY_STRUCTURE.md](docs/REPOSITORY_STRUCTURE.md) | Detailed folder explanation |



---

## ğŸ“„ License

MIT License - see [LICENSE](alpamayo/LICENSE) for details.

---

## ğŸ™ Acknowledgments

- ESV Competition organizers
- Ultralytics for YOLOv12
- OpenAI for CLIP
- PaddleOCR and EasyOCR teams
- W&B for experiment tracking

---

## ğŸ“§ Contact

For questions or feedback:
- **GitHub Issues**: [github.com/WMaia9/workzone/issues](https://github.com/WMaia9/workzone/issues)

---

**Built with â¤ï¸ for safer roads**
