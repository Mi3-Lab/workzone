{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af39439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /data/wesleyferreiramaia/workzone\n",
      "VIDEOS_DIR exists: True /data/wesleyferreiramaia/workzone/data/videos_compressed\n"
     ]
    }
   ],
   "source": [
    "# Alpamayo-R1 on YOUR workzone videos (state classification)\n",
    "\n",
    "import os, sys, re, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "ROOT = Path(\"..\").resolve()          # you run from <root>/notebooks\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "VIDEOS_DIR = DATA_DIR / \"videos_compressed\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"VIDEOS_DIR exists:\", VIDEOS_DIR.exists(), VIDEOS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6afd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your repo has src/ with alpamayo_r1 inside, keep this:\n",
    "if os.path.abspath(\"src\") not in sys.path:\n",
    "    sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "from alpamayo_r1.models.alpamayo_r1 import AlpamayoR1\n",
    "from alpamayo_r1.load_physical_aiavdataset import load_physical_aiavdataset\n",
    "from alpamayo_r1 import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387f7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda | DTYPE: torch.bfloat16\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26c3a91be6b4080bcf5c7c8da9b04f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + processor loaded.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE  = torch.bfloat16 if DEVICE == \"cuda\" else torch.float16\n",
    "\n",
    "MODEL_ID = \"nvidia/Alpamayo-R1-10B\"\n",
    "\n",
    "print(\"DEVICE:\", DEVICE, \"| DTYPE:\", DTYPE)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AlpamayoR1.from_pretrained(MODEL_ID, dtype=DTYPE).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "processor = helper.get_processor(model.tokenizer)\n",
    "print(\"Model + processor loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19b163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading template data to steal shapes...\n",
      "Template keys: dict_keys(['image_frames', 'camera_indices', 'ego_history_xyz', 'ego_history_rot', 'ego_future_xyz', 'ego_future_rot', 'relative_timestamps', 'absolute_timestamps', 't0_us', 'clip_id'])\n",
      "image_frames: torch.Size([4, 4, 3, 1080, 1920]) torch.uint8\n",
      "ego_history_xyz: torch.Size([1, 1, 16, 3]) torch.float32\n",
      "ego_history_rot: torch.Size([1, 1, 16, 3, 3]) torch.float32\n",
      "ego_future_xyz: torch.Size([1, 1, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "# Any valid clip_id you already used successfully:\n",
    "TEMPLATE_CLIP_ID = \"030c760c-ae38-49aa-9ad8-f5650a545d26\"\n",
    "\n",
    "print(\"Loading template data to steal shapes...\")\n",
    "tmpl = load_physical_aiavdataset(TEMPLATE_CLIP_ID, t0_us=5_100_000)\n",
    "\n",
    "print(\"Template keys:\", tmpl.keys())\n",
    "print(\"image_frames:\", tmpl[\"image_frames\"].shape, tmpl[\"image_frames\"].dtype)\n",
    "print(\"ego_history_xyz:\", tmpl[\"ego_history_xyz\"].shape, tmpl[\"ego_history_xyz\"].dtype)\n",
    "print(\"ego_history_rot:\", tmpl[\"ego_history_rot\"].shape, tmpl[\"ego_history_rot\"].dtype)\n",
    "print(\"ego_future_xyz:\", tmpl[\"ego_future_xyz\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e4f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: san_antonio.mp4\n",
      "Thought: [['Keep lane to continue driving since the lane ahead is clear.']]\n",
      "Thought: [['Keep lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Slow down to stop at the stop sign ahead.']]\n",
      "Thought: [['Stop for the stop sign at the intersection.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction equipment on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction worker on the right.']]\n",
      "Thought: [['Nudge left to increase clearance from the construction worker on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones intruding into the lane.']]\n",
      "Thought: [['Nudge to the left to avoid the cone in the same lane.']]\n",
      "Saved: alpamayo_result.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Robust Text Cleaner ---\n",
    "def clean_and_wrap_text(raw_text, width=60):\n",
    "    # Unwrap nested lists recursively (handles [['Text']] or even [[['Text']]])\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "            \n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "        \n",
    "    text = str(raw_text)\n",
    "    \n",
    "    # Remove special tokens and prompt echoes\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "        \n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Wrap text into multiple lines for the video\n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "# --- 2. Main Loop ---\n",
    "def run_final_visualization(video_path, model, processor, tmpl, out_path, stride=3):\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # \"Cruise Control\" History (Neutralizing the Ghost Driver)\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    print(f\"Processing: {video_path.name}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames_bgr = []\n",
    "            sampled_rgb = []\n",
    "            for i in range(int(T1*T2) * stride):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                frames_bgr.append(frame)\n",
    "                if i % stride == 0:\n",
    "                    fr = cv2.resize(frame, (tW, tH))\n",
    "                    fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
    "                    sampled_rgb.append(fr)\n",
    "            \n",
    "            if len(sampled_rgb) != int(T1*T2): break \n",
    "\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            # --- Native Prompt ---\n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            \n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_dict=True, \n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            # Inference\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    max_generation_length=256,\n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # --- Clean Data for Display ---\n",
    "            raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "            lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "            \n",
    "            # Print FULL text to console (No cutting!)\n",
    "            print(f\"Thought: {full_text}\")\n",
    "\n",
    "            # Draw on Video\n",
    "            for fr in frames_bgr:\n",
    "                # Black banner at the top\n",
    "                banner_height = 50 + (len(lines) * 35)\n",
    "                cv2.rectangle(fr, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "                \n",
    "                # Title\n",
    "                cv2.putText(fr, \"ALPAYMAO REASONING:\", (20, 35), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "                \n",
    "                # Draw lines\n",
    "                y = 80\n",
    "                for line in lines:\n",
    "                    cv2.putText(fr, line, (20, y), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    y += 35\n",
    "                \n",
    "                out.write(fr)\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "run_final_visualization(\n",
    "    video_path=Path(\"/home/wesleyferreiramaia/data/workzone/data/demo/san_antonio.mp4\"),\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    tmpl=tmpl,\n",
    "    out_path=Path(\"alpamayo_result.mp4\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d090557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Found 16 videos in demo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f932caba01a24c978bb102162a7004ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Video:', layout=Layout(width='60%'), options=('boston_workzone_short.mp4'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d243f38d04044ecb94c3804605fc7e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "# --- Configuration ---\n",
    "VIDEO_DIR = Path(\"/home/wesleyferreiramaia/data/workzone/data/demo/\")\n",
    "\n",
    "# --- 1. Helper: Text Cleaning ---\n",
    "def clean_and_wrap_text(raw_text, width=65):\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "            \n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "        \n",
    "    text = str(raw_text)\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "        \n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    \n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "# --- 2. Inference Function (Optimized) ---\n",
    "def run_live_inference(video_path, model, processor, tmpl, out_path, stride=3):\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_video = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps_video, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # \"Cruising\" History Fix (Neutralizes Ghost Driver)\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    print(f\"Starting Live Inference on: {video_path.name}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            t_start = time.time() # Start Timer\n",
    "            \n",
    "            frames_bgr = []\n",
    "            sampled_rgb = []\n",
    "            for i in range(int(T1*T2) * stride):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                frames_bgr.append(frame)\n",
    "                if i % stride == 0:\n",
    "                    fr = cv2.resize(frame, (tW, tH))\n",
    "                    fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
    "                    sampled_rgb.append(fr)\n",
    "            \n",
    "            if len(sampled_rgb) != int(T1*T2): break \n",
    "\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            # Prepare Prompt\n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            \n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_dict=True, \n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            # Inference Call\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    # OPTIMIZATION: Reduced from 256 to 64 for speed\n",
    "                    max_generation_length=64,  \n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # Process Output\n",
    "            raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "            lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "            \n",
    "            # Calculate FPS\n",
    "            t_end = time.time()\n",
    "            fps_val = 1.0 / (t_end - t_start)\n",
    "\n",
    "            # Draw on Video Frames\n",
    "            for fr in frames_bgr:\n",
    "                # Black Banner\n",
    "                banner_height = 60 + (len(lines) * 35)\n",
    "                cv2.rectangle(fr, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "                \n",
    "                # Title & Speed\n",
    "                cv2.putText(fr, f\"ALPAYMAO REASONING ({fps_val:.2f} FPS):\", (20, 35), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                \n",
    "                # Reasoning Text\n",
    "                y = 80\n",
    "                for line in lines:\n",
    "                    cv2.putText(fr, line, (20, y), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    y += 35\n",
    "                \n",
    "                out.write(fr)\n",
    "\n",
    "            # Live Display Update\n",
    "            _, encoded_img = cv2.imencode('.jpg', frames_bgr[-1]) \n",
    "            clear_output(wait=True)\n",
    "            display(Image(data=encoded_img))\n",
    "            print(f\"âš¡ Speed: {fps_val:.2f} FPS | Video: {video_path.name}\")\n",
    "            print(f\"Thought: {full_text}\")\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"\\nâœ… Video saved to: {out_path}\")\n",
    "\n",
    "# --- 3. Interactive Dashboard ---\n",
    "def launch_interactive_dashboard():\n",
    "    # Scan for videos\n",
    "    video_files = sorted([f.name for f in VIDEO_DIR.glob(\"*.mp4\")])\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No MP4 videos found in {VIDEO_DIR}\")\n",
    "        return\n",
    "\n",
    "    # Create Widgets\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=video_files,\n",
    "        description='Video:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='60%')\n",
    "    )\n",
    "    \n",
    "    btn_run = widgets.Button(\n",
    "        description='â–¶ Run Model',\n",
    "        button_style='success',\n",
    "        icon='play',\n",
    "        layout=widgets.Layout(width='20%')\n",
    "    )\n",
    "    \n",
    "    out_log = widgets.Output()\n",
    "\n",
    "    # Button Action\n",
    "    def on_button_click(b):\n",
    "        with out_log:\n",
    "            clear_output() \n",
    "            \n",
    "            selected_video = VIDEO_DIR / dropdown.value\n",
    "            \n",
    "            # Dynamic Naming: my_video.mp4 -> my_video_alpamayo.mp4\n",
    "            new_name = f\"{selected_video.stem}_alpamayo.mp4\"\n",
    "            output_path = Path(new_name)\n",
    "            \n",
    "            print(f\"ðŸš€ Loading: {selected_video.name}...\")\n",
    "            print(f\"ðŸ’¾ Output will be saved to: {new_name}\")\n",
    "            \n",
    "            try:\n",
    "                run_live_inference(\n",
    "                    video_path=selected_video,\n",
    "                    model=model,\n",
    "                    processor=processor,\n",
    "                    tmpl=tmpl,\n",
    "                    out_path=output_path,\n",
    "                    stride=3\n",
    "                )\n",
    "                print(f\"\\nâœ… Finished! Saved to {new_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Error: {e}\")\n",
    "\n",
    "    btn_run.on_click(on_button_click)\n",
    "\n",
    "    # Display\n",
    "    print(f\"ðŸ“‚ Found {len(video_files)} videos in {VIDEO_DIR.name}\")\n",
    "    display(widgets.HBox([dropdown, btn_run]))\n",
    "    display(out_log)\n",
    "\n",
    "# --- Start ---\n",
    "launch_interactive_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efd649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0500530e661b4c1f9f2d56b33ad3b600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Video:', layout=Layout(width='60%'), options=('boston_workzone_short.mp4'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "# --- Configuration ---\n",
    "VIDEO_DIR = Path(\"/home/wesleyferreiramaia/data/workzone/data/demo/\")\n",
    "\n",
    "def clean_and_wrap_text(raw_text, width=65):\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "    text = str(raw_text)\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "def run_live_inference(video_path, model, processor, tmpl, out_path, stride=10): # <--- Default stride increased\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_video = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps_video, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # Cruising History\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    print(f\"Starting High-Speed Inference on: {video_path.name}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            t_start = time.time()\n",
    "            \n",
    "            frames_bgr = []\n",
    "            sampled_rgb = []\n",
    "            \n",
    "            # --- SPEED HACK: Process bigger chunks of video at once ---\n",
    "            # Stride determines how many frames we read per loop.\n",
    "            # Higher stride = More frames processed per model call = Higher FPS\n",
    "            frames_to_read = int(T1*T2) * stride\n",
    "            \n",
    "            for i in range(frames_to_read):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                frames_bgr.append(frame)\n",
    "                \n",
    "                # We still only sample a few frames for the model to see\n",
    "                if i % stride == 0 and len(sampled_rgb) < int(T1*T2):\n",
    "                    fr = cv2.resize(frame, (tW, tH))\n",
    "                    fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
    "                    sampled_rgb.append(fr)\n",
    "            \n",
    "            if len(frames_bgr) == 0: break \n",
    "\n",
    "            # Pad if we ran out of frames early\n",
    "            while len(sampled_rgb) < int(T1*T2):\n",
    "                sampled_rgb.append(sampled_rgb[-1])\n",
    "\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            # Prompt\n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, tokenize=True, add_generation_prompt=True,\n",
    "                return_dict=True, return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    max_generation_length=40,  # <--- SPEED HACK: Extremely short reasoning (40 tokens)\n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # Process Output\n",
    "            raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "            lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "            \n",
    "            # Draw on ALL frames in the batch (Visual Persistence)\n",
    "            for fr in frames_bgr:\n",
    "                banner_height = 60 + (len(lines) * 35)\n",
    "                cv2.rectangle(fr, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "                \n",
    "                t_now = time.time()\n",
    "                current_fps = len(frames_bgr) / (t_now - t_start + 1e-5)\n",
    "                \n",
    "                cv2.putText(fr, f\"ALPAMAYO ({current_fps:.1f} FPS):\", (20, 35), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                \n",
    "                y = 80\n",
    "                for line in lines:\n",
    "                    cv2.putText(fr, line, (20, y), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    y += 35\n",
    "                out.write(fr)\n",
    "\n",
    "            # Live Display\n",
    "            _, encoded_img = cv2.imencode('.jpg', frames_bgr[-1]) \n",
    "            clear_output(wait=True)\n",
    "            display(Image(data=encoded_img))\n",
    "            \n",
    "            # Print Stats\n",
    "            fps_val = len(frames_bgr) / (time.time() - t_start)\n",
    "            print(f\"âš¡ Speed: {fps_val:.2f} FPS (Batch: {len(frames_bgr)} frames)\")\n",
    "            print(f\"Thought: {full_text}\")\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"\\nâœ… Video saved to: {out_path}\")\n",
    "\n",
    "def launch_interactive_dashboard():\n",
    "    video_files = sorted([f.name for f in VIDEO_DIR.glob(\"*.mp4\")])\n",
    "    if not video_files: return\n",
    "\n",
    "    dropdown = widgets.Dropdown(options=video_files, description='Video:', layout=widgets.Layout(width='60%'))\n",
    "    \n",
    "    # Slider for Speed Control\n",
    "    slider_stride = widgets.IntSlider(\n",
    "        value=15, min=3, max=30, step=1, \n",
    "        description='Speed (Stride):', \n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    btn_run = widgets.Button(description='â–¶ Run', button_style='success', icon='play')\n",
    "    out_log = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with out_log:\n",
    "            clear_output()\n",
    "            selected_video = VIDEO_DIR / dropdown.value\n",
    "            new_name = f\"{selected_video.stem}_alpamayo_fast.mp4\"\n",
    "            \n",
    "            print(f\"ðŸš€ Loading with Stride {slider_stride.value}...\")\n",
    "            run_live_inference(\n",
    "                video_path=selected_video,\n",
    "                model=model,\n",
    "                processor=processor,\n",
    "                tmpl=tmpl,\n",
    "                out_path=Path(new_name),\n",
    "                stride=slider_stride.value # <--- Use the slider value\n",
    "            )\n",
    "\n",
    "    btn_run.on_click(on_button_click)\n",
    "    display(widgets.VBox([dropdown, slider_stride, btn_run, out_log]))\n",
    "\n",
    "launch_interactive_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35743207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9448b6fa76c44f6b7ba676987f70ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Video:', layout=Layout(width='60%'), options=('boston_workzone_short.mp4'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "# --- Configuration ---\n",
    "VIDEO_DIR = Path(\"/home/wesleyferreiramaia/data/workzone/data/demo/\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def clean_and_wrap_text(raw_text, width=65):\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "    text = str(raw_text)\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "def run_async_simulation(video_path, model, processor, tmpl, out_path, reasoning_interval=10):\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_video = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps_video, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # Cruising History\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    # --- STATE VARIABLES ---\n",
    "    # These represent the \"Memory\" of the onboard computer\n",
    "    latest_reasoning_lines = [\"Initializing System...\"]\n",
    "    frame_counter = 0\n",
    "\n",
    "    print(f\"Starting Async Architecture Simulation...\")\n",
    "    print(f\" - Action Update: Every Frame (100% Speed)\")\n",
    "    print(f\" - Logic Update:  Every {reasoning_interval} Frames (~10% Speed)\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # 1. Read ONE frame (Simulating Real-Time Sensor Stream)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            \n",
    "            # 2. Prepare Inputs\n",
    "            fr_resized = cv2.resize(frame, (tW, tH))\n",
    "            fr_rgb = cv2.cvtColor(fr_resized, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Repeat this frame to fill the context window (Simulates \"current moment\")\n",
    "            # In a real car, this would be a buffer of the last 3 frames\n",
    "            sampled_rgb = [fr_rgb] * int(T1*T2)\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, tokenize=True, add_generation_prompt=True,\n",
    "                return_dict=True, return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            # --- 3. THE ASYNC LOGIC ---\n",
    "            t_start = time.time()\n",
    "            \n",
    "            # DECISION: Do we run the heavy \"Reasoning\" engine this frame?\n",
    "            run_reasoning = (frame_counter % reasoning_interval == 0)\n",
    "            \n",
    "            # Set Token Limit based on mode\n",
    "            # FAST MODE: 1 token (Just enough to trigger trajectory head)\n",
    "            # SLOW MODE: 64 tokens (Enough to explain the scene)\n",
    "            current_max_tokens = 64 if run_reasoning else 1\n",
    "\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    max_generation_length=current_max_tokens, \n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # --- 4. UPDATE STATE ---\n",
    "            # Trajectory is ALWAYS fresh (calculated every frame)\n",
    "            # Reasoning is ONLY updated if we ran the heavy engine\n",
    "            if run_reasoning:\n",
    "                raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "                lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "                latest_reasoning_lines = lines # Update the \"Dashboard\"\n",
    "                status_color = (0, 255, 0) # Green dot = New Thought\n",
    "            else:\n",
    "                status_color = (0, 0, 255) # Red dot = Stale/Cached Thought\n",
    "\n",
    "            # --- 5. VISUALIZATION ---\n",
    "            # Draw the Dashboard\n",
    "            banner_height = 60 + (len(latest_reasoning_lines) * 35)\n",
    "            cv2.rectangle(frame, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "            \n",
    "            # Indicator Light (Simulates Async Thread Activity)\n",
    "            cv2.circle(frame, (W-50, 40), 15, status_color, -1)\n",
    "            cv2.putText(frame, \"CPU\", (W-80, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200,200,200), 1)\n",
    "\n",
    "            cv2.putText(frame, \"ONBOARD REASONING:\", (20, 35), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            \n",
    "            y = 80\n",
    "            for line in latest_reasoning_lines:\n",
    "                cv2.putText(frame, line, (20, y), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                y += 35\n",
    "            \n",
    "            out.write(frame)\n",
    "\n",
    "            # Display Logic\n",
    "            if frame_counter % 2 == 0: # Update display every other frame to save bandwidth\n",
    "                _, encoded_img = cv2.imencode('.jpg', frame) \n",
    "                clear_output(wait=True)\n",
    "                display(Image(data=encoded_img))\n",
    "                fps_val = 1.0 / (time.time() - t_start)\n",
    "                print(f\"âš¡ Mode: {'REASONING UPDATE' if run_reasoning else 'ACTION ONLY'}\")\n",
    "                print(f\"âš¡ Speed: {fps_val:.2f} FPS\")\n",
    "\n",
    "            frame_counter += 1\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"\\nâœ… Simulation saved to: {out_path}\")\n",
    "\n",
    "# --- Launch ---\n",
    "def launch_async_dashboard():\n",
    "    video_files = sorted([f.name for f in VIDEO_DIR.glob(\"*.mp4\")])\n",
    "    if not video_files: return\n",
    "    dropdown = widgets.Dropdown(options=video_files, description='Video:', layout=widgets.Layout(width='60%'))\n",
    "    btn_run = widgets.Button(description='â–¶ Run Simulation', button_style='warning', icon='cogs')\n",
    "    out_log = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with out_log:\n",
    "            clear_output()\n",
    "            selected_video = VIDEO_DIR / dropdown.value\n",
    "            new_name = f\"{selected_video.stem}_async_sim.mp4\"\n",
    "            print(f\"ðŸš€ Simulating Onboard Computer for: {selected_video.name}...\")\n",
    "            run_async_simulation(\n",
    "                video_path=selected_video,\n",
    "                model=model,\n",
    "                processor=processor,\n",
    "                tmpl=tmpl,\n",
    "                out_path=Path(new_name),\n",
    "                reasoning_interval=10 # Update text every 10 frames\n",
    "            )\n",
    "\n",
    "    btn_run.on_click(on_button_click)\n",
    "    display(widgets.VBox([dropdown, btn_run, out_log]))\n",
    "\n",
    "launch_async_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702e616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
