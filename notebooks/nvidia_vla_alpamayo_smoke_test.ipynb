{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af39439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /data/wesleyferreiramaia/workzone\n",
      "VIDEOS_DIR exists: True /data/wesleyferreiramaia/workzone/data/videos_compressed\n"
     ]
    }
   ],
   "source": [
    "# Alpamayo-R1 on YOUR workzone videos (state classification)\n",
    "\n",
    "import os, sys, re, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "ROOT = Path(\"..\").resolve()          # you run from <root>/notebooks\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "VIDEOS_DIR = DATA_DIR / \"videos_compressed\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"VIDEOS_DIR exists:\", VIDEOS_DIR.exists(), VIDEOS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6afd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your repo has src/ with alpamayo_r1 inside, keep this:\n",
    "if os.path.abspath(\"src\") not in sys.path:\n",
    "    sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "from alpamayo_r1.models.alpamayo_r1 import AlpamayoR1\n",
    "from alpamayo_r1.load_physical_aiavdataset import load_physical_aiavdataset\n",
    "from alpamayo_r1 import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387f7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda | DTYPE: torch.bfloat16\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ce394c967b45798c5c84db745ba685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.55 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDEVICE:\u001b[39m\u001b[33m\"\u001b[39m, DEVICE, \u001b[33m\"\u001b[39m\u001b[33m| DTYPE:\u001b[39m\u001b[33m\"\u001b[39m, DTYPE)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mAlpamayoR1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model.eval()\n\u001b[32m     12\u001b[39m processor = helper.get_processor(model.tokenizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/wesleyferreiramaia/workzone/alpamayo/ar1_venv/lib64/python3.12/site-packages/transformers/modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/wesleyferreiramaia/workzone/alpamayo/ar1_venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/wesleyferreiramaia/workzone/alpamayo/ar1_venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/wesleyferreiramaia/workzone/alpamayo/ar1_venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 928 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/wesleyferreiramaia/workzone/alpamayo/ar1_venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/wesleyferreiramaia/workzone/alpamayo/ar1_venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/wesleyferreiramaia/workzone/alpamayo/ar1_venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.55 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE  = torch.bfloat16 if DEVICE == \"cuda\" else torch.float16\n",
    "\n",
    "MODEL_ID = \"nvidia/Alpamayo-R1-10B\"\n",
    "\n",
    "print(\"DEVICE:\", DEVICE, \"| DTYPE:\", DTYPE)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AlpamayoR1.from_pretrained(MODEL_ID, dtype=DTYPE).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "processor = helper.get_processor(model.tokenizer)\n",
    "print(\"Model + processor loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa68c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19b163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading template data to steal shapes...\n",
      "Template keys: dict_keys(['image_frames', 'camera_indices', 'ego_history_xyz', 'ego_history_rot', 'ego_future_xyz', 'ego_future_rot', 'relative_timestamps', 'absolute_timestamps', 't0_us', 'clip_id'])\n",
      "image_frames: torch.Size([4, 4, 3, 1080, 1920]) torch.uint8\n",
      "ego_history_xyz: torch.Size([1, 1, 16, 3]) torch.float32\n",
      "ego_history_rot: torch.Size([1, 1, 16, 3, 3]) torch.float32\n",
      "ego_future_xyz: torch.Size([1, 1, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "# Any valid clip_id you already used successfully:\n",
    "TEMPLATE_CLIP_ID = \"030c760c-ae38-49aa-9ad8-f5650a545d26\"\n",
    "\n",
    "print(\"Loading template data to steal shapes...\")\n",
    "tmpl = load_physical_aiavdataset(TEMPLATE_CLIP_ID, t0_us=5_100_000)\n",
    "\n",
    "print(\"Template keys:\", tmpl.keys())\n",
    "print(\"image_frames:\", tmpl[\"image_frames\"].shape, tmpl[\"image_frames\"].dtype)\n",
    "print(\"ego_history_xyz:\", tmpl[\"ego_history_xyz\"].shape, tmpl[\"ego_history_xyz\"].dtype)\n",
    "print(\"ego_history_rot:\", tmpl[\"ego_history_rot\"].shape, tmpl[\"ego_history_rot\"].dtype)\n",
    "print(\"ego_future_xyz:\", tmpl[\"ego_future_xyz\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e4f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: EYosemiteAve_NightFog.mp4\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light is green.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Lane change to the left to remain in the through lane and avoid being forced into the right-turn/exit lane, while keeping a safe gap to the lead vehicle on a wet road.']]\n",
      "Thought: [['Change lanes to the right to pass the slower lead vehicle ahead while the adjacent right lane is clear and maintain safe clearance from the snowy curb.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Resume speed from stop at the stop sign.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is slowing ahead.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is slowing ahead.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is slowing ahead.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is slowing ahead.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is stopped ahead.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Stop to keep distance to the lead vehicle']]\n",
      "Thought: [['Stop to keep distance to the lead vehicle.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is slowing ahead.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Change lanes to the right to pass the slower lead vehicle and maintain progress, using the clear adjacent lane and keeping safe clearance to the curb.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle to maintain a safe gap.']]\n",
      "Thought: [['Nudge to the left to avoid the traffic cone in the lane.']]\n",
      "Thought: [['Nudge to the left to avoid the road debris in the same lane.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Adapt speed for the right curve since the road bends right ahead.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since the path ahead is clear.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is moving ahead in the same path.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Yield to the crossing vehicle ahead since it is moving across the path in front.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Adapt speed for the right-hand curve ahead.']]\n",
      "Thought: [['Adapt speed for the right curve since the driveway bends right.']]\n",
      "Thought: [['Adapt speed for the speed bump ahead.']]\n",
      "Thought: [['Nudge to the right to increase clearance from the road debris ahead.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since the lane ahead is clear.']]\n",
      "Thought: [['Keep lane to continue driving since the road ahead is clear.']]\n",
      "Thought: [['Keep lane to continue driving since the path ahead is clear.']]\n",
      "Thought: [['Keep lane to continue driving since the road ahead is clear.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since the road ahead is clear.']]\n",
      "Thought: [['Adapt speed to slow for the right curve ahead.']]\n",
      "Thought: [['Stop for the stop sign ahead.']]\n",
      "Thought: [['Stop for the stop sign.']]\n",
      "Thought: [['Turn right at the intersection to follow the road.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Adapt speed to follow the right curve ahead.']]\n",
      "Thought: [['Slow down to keep distance to the lead vehicle.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Stop for the red traffic light.']]\n",
      "Thought: [['Stop at the stop line because the right-turn traffic light is red.']]\n",
      "Thought: [['Turn right at the intersection since the right-turn traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Turn right at the intersection since the right-turn traffic light turns green.']]\n",
      "Thought: [['Nudge left to increase clearance from the parked trailer ahead.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the parked truck encroaching into the lane.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Slow down for the red traffic light ahead.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Slow down for the red traffic light.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Stop for the red traffic light.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Adjust speed due to the rightward road curvature.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep distance to the lead vehicle since it is directly ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is ahead in the same lane.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cone ahead.']]\n",
      "Thought: [['Slow down to approach the intersection since the straight traffic light is red.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Thought: [['Slow down to keep distance to the lead vehicle ahead.']]\n",
      "Thought: [['Slow down for the red traffic light ahead.']]\n",
      "Thought: [['Stop for the red traffic light.']]\n",
      "Thought: [['Stop for the red traffic light.']]\n",
      "Thought: [['Stop for the red traffic light.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Stop for the red traffic light.']]\n",
      "Thought: [['Accelerate to proceed through the intersection since the straight traffic light turns green.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cone ahead.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the traffic cones on the right edge of the lane.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cone in the lane.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep lane to maintain clearance from the work-zone cones on the right.']]\n",
      "Thought: [['Keep lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cone on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Keep distance to the lead vehicle because a vehicle is ahead in the same lane.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cones on the right.']]\n",
      "Thought: [['Keep at the center of the lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep lane to continue through the work zone since the cones line the lane edge.']]\n",
      "Thought: [['Keep lane to follow the work zone cones']]\n",
      "Thought: [['Keep lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep lane to continue driving since no critical agent needs attention.']]\n",
      "Thought: [['Keep distance to the lead vehicle ahead in the same lane.']]\n",
      "Thought: [['Keep lane to continue driving since the road ahead is clear.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the traffic cones on the right edge of the lane.']]\n",
      "Thought: [['Nudge to the left to increase clearance from the construction cone ahead.']]\n",
      "Thought: [['Stop for the stop sign ahead.']]\n",
      "Thought: [['Stop at the stop line for the stop sign.']]\n",
      "Thought: [['Yield to the cross-traffic vehicle because it is crossing ahead at the intersection.']]\n",
      "Thought: [['Yield to the cross-traffic vehicle crossing ahead.']]\n",
      "Thought: [['Yield to the cross-traffic vehicle crossing ahead.']]\n",
      "Thought: [['Yield to the cross-traffic because vehicles are crossing ahead.']]\n",
      "Thought: [['Keep distance to the lead vehicle because it is directly ahead in the same lane.']]\n",
      "Saved: aEYosemiteAve_NightFog_alpamayo.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Robust Text Cleaner ---\n",
    "def clean_and_wrap_text(raw_text, width=60):\n",
    "    # Unwrap nested lists recursively (handles [['Text']] or even [[['Text']]])\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "            \n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "        \n",
    "    text = str(raw_text)\n",
    "    \n",
    "    # Remove special tokens and prompt echoes\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "        \n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Wrap text into multiple lines for the video\n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "# --- 2. Main Loop ---\n",
    "def run_final_visualization(video_path, model, processor, tmpl, out_path, stride=3):\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # \"Cruise Control\" History (Neutralizing the Ghost Driver)\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    print(f\"Processing: {video_path.name}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames_bgr = []\n",
    "            sampled_rgb = []\n",
    "            for i in range(int(T1*T2) * stride):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                frames_bgr.append(frame)\n",
    "                if i % stride == 0:\n",
    "                    fr = cv2.resize(frame, (tW, tH))\n",
    "                    fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
    "                    sampled_rgb.append(fr)\n",
    "            \n",
    "            if len(sampled_rgb) != int(T1*T2): break \n",
    "\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            # --- Native Prompt ---\n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            \n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_dict=True, \n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            # Inference\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    max_generation_length=256,\n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # --- Clean Data for Display ---\n",
    "            raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "            lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "            \n",
    "            # Print FULL text to console (No cutting!)\n",
    "            print(f\"Thought: {full_text}\")\n",
    "\n",
    "            # Draw on Video\n",
    "            for fr in frames_bgr:\n",
    "                # Black banner at the top\n",
    "                banner_height = 50 + (len(lines) * 35)\n",
    "                cv2.rectangle(fr, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "                \n",
    "                # Title\n",
    "                cv2.putText(fr, \"ALPAYMAO REASONING:\", (20, 35), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "                \n",
    "                # Draw lines\n",
    "                y = 80\n",
    "                for line in lines:\n",
    "                    cv2.putText(fr, line, (20, y), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    y += 35\n",
    "                \n",
    "                out.write(fr)\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "run_final_visualization(\n",
    "    video_path=Path(\"/home/wesleyferreiramaia/data/workzone/alpamayo/EYosemiteAve_NightFog.mp4\"),\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    tmpl=tmpl,\n",
    "    out_path=Path(\"aEYosemiteAve_NightFog_alpamayo.mp4\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d090557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Found 16 videos in demo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afb2b794c6048ee9fec6297a4da0a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Video:', layout=Layout(width='60%'), options=('boston_workzone_short.mp4'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad06a4c63557444da690d97907eb0828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "# --- Configuration ---\n",
    "VIDEO_DIR = Path(\"/home/wesleyferreiramaia/data/workzone/data/demo/\")\n",
    "\n",
    "# --- 1. Helper: Text Cleaning ---\n",
    "def clean_and_wrap_text(raw_text, width=65):\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "            \n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "        \n",
    "    text = str(raw_text)\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "        \n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    \n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "# --- 2. Inference Function (Optimized) ---\n",
    "def run_live_inference(video_path, model, processor, tmpl, out_path, stride=3):\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_video = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps_video, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # \"Cruising\" History Fix (Neutralizes Ghost Driver)\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    print(f\"Starting Live Inference on: {video_path.name}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            t_start = time.time() # Start Timer\n",
    "            \n",
    "            frames_bgr = []\n",
    "            sampled_rgb = []\n",
    "            for i in range(int(T1*T2) * stride):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                frames_bgr.append(frame)\n",
    "                if i % stride == 0:\n",
    "                    fr = cv2.resize(frame, (tW, tH))\n",
    "                    fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
    "                    sampled_rgb.append(fr)\n",
    "            \n",
    "            if len(sampled_rgb) != int(T1*T2): break \n",
    "\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            # Prepare Prompt\n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            \n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_dict=True, \n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            # Inference Call\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    # OPTIMIZATION: Reduced from 256 to 64 for speed\n",
    "                    max_generation_length=64,  \n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # Process Output\n",
    "            raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "            lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "            \n",
    "            # Calculate FPS\n",
    "            t_end = time.time()\n",
    "            fps_val = 1.0 / (t_end - t_start)\n",
    "\n",
    "            # Draw on Video Frames\n",
    "            for fr in frames_bgr:\n",
    "                # Black Banner\n",
    "                banner_height = 60 + (len(lines) * 35)\n",
    "                cv2.rectangle(fr, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "                \n",
    "                # Title & Speed\n",
    "                cv2.putText(fr, f\"ALPAYMAO REASONING ({fps_val:.2f} FPS):\", (20, 35), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                \n",
    "                # Reasoning Text\n",
    "                y = 80\n",
    "                for line in lines:\n",
    "                    cv2.putText(fr, line, (20, y), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    y += 35\n",
    "                \n",
    "                out.write(fr)\n",
    "\n",
    "            # Live Display Update\n",
    "            _, encoded_img = cv2.imencode('.jpg', frames_bgr[-1]) \n",
    "            clear_output(wait=True)\n",
    "            display(Image(data=encoded_img))\n",
    "            print(f\"âš¡ Speed: {fps_val:.2f} FPS | Video: {video_path.name}\")\n",
    "            print(f\"Thought: {full_text}\")\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"\\nâœ… Video saved to: {out_path}\")\n",
    "\n",
    "# --- 3. Interactive Dashboard ---\n",
    "def launch_interactive_dashboard():\n",
    "    # Scan for videos\n",
    "    video_files = sorted([f.name for f in VIDEO_DIR.glob(\"*.mp4\")])\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No MP4 videos found in {VIDEO_DIR}\")\n",
    "        return\n",
    "\n",
    "    # Create Widgets\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=video_files,\n",
    "        description='Video:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='60%')\n",
    "    )\n",
    "    \n",
    "    btn_run = widgets.Button(\n",
    "        description='â–¶ Run Model',\n",
    "        button_style='success',\n",
    "        icon='play',\n",
    "        layout=widgets.Layout(width='20%')\n",
    "    )\n",
    "    \n",
    "    out_log = widgets.Output()\n",
    "\n",
    "    # Button Action\n",
    "    def on_button_click(b):\n",
    "        with out_log:\n",
    "            clear_output() \n",
    "            \n",
    "            selected_video = VIDEO_DIR / dropdown.value\n",
    "            \n",
    "            # Dynamic Naming: my_video.mp4 -> my_video_alpamayo.mp4\n",
    "            new_name = f\"{selected_video.stem}_alpamayo.mp4\"\n",
    "            output_path = Path(new_name)\n",
    "            \n",
    "            print(f\"ðŸš€ Loading: {selected_video.name}...\")\n",
    "            print(f\"ðŸ’¾ Output will be saved to: {new_name}\")\n",
    "            \n",
    "            try:\n",
    "                run_live_inference(\n",
    "                    video_path=selected_video,\n",
    "                    model=model,\n",
    "                    processor=processor,\n",
    "                    tmpl=tmpl,\n",
    "                    out_path=output_path,\n",
    "                    stride=3\n",
    "                )\n",
    "                print(f\"\\nâœ… Finished! Saved to {new_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Error: {e}\")\n",
    "\n",
    "    btn_run.on_click(on_button_click)\n",
    "\n",
    "    # Display\n",
    "    print(f\"ðŸ“‚ Found {len(video_files)} videos in {VIDEO_DIR.name}\")\n",
    "    display(widgets.HBox([dropdown, btn_run]))\n",
    "    display(out_log)\n",
    "\n",
    "# --- Start ---\n",
    "launch_interactive_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efd649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0500530e661b4c1f9f2d56b33ad3b600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Video:', layout=Layout(width='60%'), options=('boston_workzone_short.mp4'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "# --- Configuration ---\n",
    "VIDEO_DIR = Path(\"/home/wesleyferreiramaia/data/workzone/data/demo/\")\n",
    "\n",
    "def clean_and_wrap_text(raw_text, width=65):\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "    text = str(raw_text)\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "def run_live_inference(video_path, model, processor, tmpl, out_path, stride=10): # <--- Default stride increased\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_video = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps_video, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # Cruising History\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    print(f\"Starting High-Speed Inference on: {video_path.name}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            t_start = time.time()\n",
    "            \n",
    "            frames_bgr = []\n",
    "            sampled_rgb = []\n",
    "            \n",
    "            # --- SPEED HACK: Process bigger chunks of video at once ---\n",
    "            # Stride determines how many frames we read per loop.\n",
    "            # Higher stride = More frames processed per model call = Higher FPS\n",
    "            frames_to_read = int(T1*T2) * stride\n",
    "            \n",
    "            for i in range(frames_to_read):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                frames_bgr.append(frame)\n",
    "                \n",
    "                # We still only sample a few frames for the model to see\n",
    "                if i % stride == 0 and len(sampled_rgb) < int(T1*T2):\n",
    "                    fr = cv2.resize(frame, (tW, tH))\n",
    "                    fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
    "                    sampled_rgb.append(fr)\n",
    "            \n",
    "            if len(frames_bgr) == 0: break \n",
    "\n",
    "            # Pad if we ran out of frames early\n",
    "            while len(sampled_rgb) < int(T1*T2):\n",
    "                sampled_rgb.append(sampled_rgb[-1])\n",
    "\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            # Prompt\n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, tokenize=True, add_generation_prompt=True,\n",
    "                return_dict=True, return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    max_generation_length=40,  # <--- SPEED HACK: Extremely short reasoning (40 tokens)\n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # Process Output\n",
    "            raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "            lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "            \n",
    "            # Draw on ALL frames in the batch (Visual Persistence)\n",
    "            for fr in frames_bgr:\n",
    "                banner_height = 60 + (len(lines) * 35)\n",
    "                cv2.rectangle(fr, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "                \n",
    "                t_now = time.time()\n",
    "                current_fps = len(frames_bgr) / (t_now - t_start + 1e-5)\n",
    "                \n",
    "                cv2.putText(fr, f\"ALPAMAYO ({current_fps:.1f} FPS):\", (20, 35), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                \n",
    "                y = 80\n",
    "                for line in lines:\n",
    "                    cv2.putText(fr, line, (20, y), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    y += 35\n",
    "                out.write(fr)\n",
    "\n",
    "            # Live Display\n",
    "            _, encoded_img = cv2.imencode('.jpg', frames_bgr[-1]) \n",
    "            clear_output(wait=True)\n",
    "            display(Image(data=encoded_img))\n",
    "            \n",
    "            # Print Stats\n",
    "            fps_val = len(frames_bgr) / (time.time() - t_start)\n",
    "            print(f\"âš¡ Speed: {fps_val:.2f} FPS (Batch: {len(frames_bgr)} frames)\")\n",
    "            print(f\"Thought: {full_text}\")\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"\\nâœ… Video saved to: {out_path}\")\n",
    "\n",
    "def launch_interactive_dashboard():\n",
    "    video_files = sorted([f.name for f in VIDEO_DIR.glob(\"*.mp4\")])\n",
    "    if not video_files: return\n",
    "\n",
    "    dropdown = widgets.Dropdown(options=video_files, description='Video:', layout=widgets.Layout(width='60%'))\n",
    "    \n",
    "    # Slider for Speed Control\n",
    "    slider_stride = widgets.IntSlider(\n",
    "        value=15, min=3, max=30, step=1, \n",
    "        description='Speed (Stride):', \n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    btn_run = widgets.Button(description='â–¶ Run', button_style='success', icon='play')\n",
    "    out_log = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with out_log:\n",
    "            clear_output()\n",
    "            selected_video = VIDEO_DIR / dropdown.value\n",
    "            new_name = f\"{selected_video.stem}_alpamayo_fast.mp4\"\n",
    "            \n",
    "            print(f\"ðŸš€ Loading with Stride {slider_stride.value}...\")\n",
    "            run_live_inference(\n",
    "                video_path=selected_video,\n",
    "                model=model,\n",
    "                processor=processor,\n",
    "                tmpl=tmpl,\n",
    "                out_path=Path(new_name),\n",
    "                stride=slider_stride.value # <--- Use the slider value\n",
    "            )\n",
    "\n",
    "    btn_run.on_click(on_button_click)\n",
    "    display(widgets.VBox([dropdown, slider_stride, btn_run, out_log]))\n",
    "\n",
    "launch_interactive_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35743207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9448b6fa76c44f6b7ba676987f70ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Video:', layout=Layout(width='60%'), options=('boston_workzone_short.mp4'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "# --- Configuration ---\n",
    "VIDEO_DIR = Path(\"/home/wesleyferreiramaia/data/workzone/data/demo/\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def clean_and_wrap_text(raw_text, width=65):\n",
    "    while isinstance(raw_text, list):\n",
    "        if len(raw_text) > 0: raw_text = raw_text[0]\n",
    "        else: raw_text = \"\"\n",
    "    if hasattr(raw_text, 'decode'): \n",
    "        raw_text = raw_text.decode(\"utf-8\", \"ignore\")\n",
    "    text = str(raw_text)\n",
    "    text = text.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    if \"assistant\" in text:\n",
    "        text = text.split(\"assistant\")[-1]\n",
    "    text = text.replace(\"Output the chain-of-thought\", \"\").replace(\"future trajectory.\", \"\")\n",
    "    text = text.strip()\n",
    "    lines = textwrap.wrap(text, width=width)\n",
    "    return lines, text\n",
    "\n",
    "def run_async_simulation(video_path, model, processor, tmpl, out_path, reasoning_interval=10):\n",
    "    video_path = Path(video_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_video = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    out = cv2.VideoWriter(str(out_path), cv2.VideoWriter_fourcc(*\"mp4v\"), fps_video, (W, H))\n",
    "    T1, T2, C, tH, tW = tmpl[\"image_frames\"].shape\n",
    "    \n",
    "    # Cruising History\n",
    "    cruise_xyz = torch.zeros_like(tmpl[\"ego_history_xyz\"])\n",
    "    cruise_rot = torch.zeros_like(tmpl[\"ego_history_rot\"])\n",
    "    cruise_rot[..., 0] = 1.0\n",
    "    for i in range(cruise_xyz.shape[1]):\n",
    "        cruise_xyz[0, i, 0] = 10.0 * ((i - 20) * 0.1)\n",
    "\n",
    "    # --- STATE VARIABLES ---\n",
    "    # These represent the \"Memory\" of the onboard computer\n",
    "    latest_reasoning_lines = [\"Initializing System...\"]\n",
    "    frame_counter = 0\n",
    "\n",
    "    print(f\"Starting Async Architecture Simulation...\")\n",
    "    print(f\" - Action Update: Every Frame (100% Speed)\")\n",
    "    print(f\" - Logic Update:  Every {reasoning_interval} Frames (~10% Speed)\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # 1. Read ONE frame (Simulating Real-Time Sensor Stream)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            \n",
    "            # 2. Prepare Inputs\n",
    "            fr_resized = cv2.resize(frame, (tW, tH))\n",
    "            fr_rgb = cv2.cvtColor(fr_resized, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Repeat this frame to fill the context window (Simulates \"current moment\")\n",
    "            # In a real car, this would be a buffer of the last 3 frames\n",
    "            sampled_rgb = [fr_rgb] * int(T1*T2)\n",
    "            tensor = torch.from_numpy(np.stack(sampled_rgb))\n",
    "            tensor = tensor.permute(0, 3, 1, 2).unsqueeze(0)\n",
    "            \n",
    "            messages = helper.create_message(tensor[0])\n",
    "            instruction_text = \"Output the chain-of-thought reasoning of the driving process, then output the future trajectory.\"\n",
    "            if isinstance(messages[0][\"content\"], list):\n",
    "                messages[0][\"content\"].append({\"type\": \"text\", \"text\": instruction_text})\n",
    "            else:\n",
    "                messages[0][\"content\"] = instruction_text\n",
    "\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages, tokenize=True, add_generation_prompt=True,\n",
    "                return_dict=True, return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            model_inputs = {\n",
    "                \"tokenized_data\": inputs,\n",
    "                \"ego_history_xyz\": cruise_xyz, \n",
    "                \"ego_history_rot\": cruise_rot,\n",
    "            }\n",
    "            model_inputs = helper.to_device(model_inputs, \"cuda\")\n",
    "\n",
    "            # --- 3. THE ASYNC LOGIC ---\n",
    "            t_start = time.time()\n",
    "            \n",
    "            # DECISION: Do we run the heavy \"Reasoning\" engine this frame?\n",
    "            run_reasoning = (frame_counter % reasoning_interval == 0)\n",
    "            \n",
    "            # Set Token Limit based on mode\n",
    "            # FAST MODE: 1 token (Just enough to trigger trajectory head)\n",
    "            # SLOW MODE: 64 tokens (Enough to explain the scene)\n",
    "            current_max_tokens = 64 if run_reasoning else 1\n",
    "\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                _, _, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "                    data=model_inputs,\n",
    "                    top_p=0.8, \n",
    "                    temperature=0.6, \n",
    "                    num_traj_samples=1,\n",
    "                    max_generation_length=current_max_tokens, \n",
    "                    return_extra=True,\n",
    "                )\n",
    "\n",
    "            # --- 4. UPDATE STATE ---\n",
    "            # Trajectory is ALWAYS fresh (calculated every frame)\n",
    "            # Reasoning is ONLY updated if we ran the heavy engine\n",
    "            if run_reasoning:\n",
    "                raw_output = extra.get(\"cot\", [\"\"])[0]\n",
    "                lines, full_text = clean_and_wrap_text(raw_output, width=65)\n",
    "                latest_reasoning_lines = lines # Update the \"Dashboard\"\n",
    "                status_color = (0, 255, 0) # Green dot = New Thought\n",
    "            else:\n",
    "                status_color = (0, 0, 255) # Red dot = Stale/Cached Thought\n",
    "\n",
    "            # --- 5. VISUALIZATION ---\n",
    "            # Draw the Dashboard\n",
    "            banner_height = 60 + (len(latest_reasoning_lines) * 35)\n",
    "            cv2.rectangle(frame, (0, 0), (W, banner_height), (0, 0, 0), -1)\n",
    "            \n",
    "            # Indicator Light (Simulates Async Thread Activity)\n",
    "            cv2.circle(frame, (W-50, 40), 15, status_color, -1)\n",
    "            cv2.putText(frame, \"CPU\", (W-80, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200,200,200), 1)\n",
    "\n",
    "            cv2.putText(frame, \"ONBOARD REASONING:\", (20, 35), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            \n",
    "            y = 80\n",
    "            for line in latest_reasoning_lines:\n",
    "                cv2.putText(frame, line, (20, y), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                y += 35\n",
    "            \n",
    "            out.write(frame)\n",
    "\n",
    "            # Display Logic\n",
    "            if frame_counter % 2 == 0: # Update display every other frame to save bandwidth\n",
    "                _, encoded_img = cv2.imencode('.jpg', frame) \n",
    "                clear_output(wait=True)\n",
    "                display(Image(data=encoded_img))\n",
    "                fps_val = 1.0 / (time.time() - t_start)\n",
    "                print(f\"âš¡ Mode: {'REASONING UPDATE' if run_reasoning else 'ACTION ONLY'}\")\n",
    "                print(f\"âš¡ Speed: {fps_val:.2f} FPS\")\n",
    "\n",
    "            frame_counter += 1\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"\\nâœ… Simulation saved to: {out_path}\")\n",
    "\n",
    "# --- Launch ---\n",
    "def launch_async_dashboard():\n",
    "    video_files = sorted([f.name for f in VIDEO_DIR.glob(\"*.mp4\")])\n",
    "    if not video_files: return\n",
    "    dropdown = widgets.Dropdown(options=video_files, description='Video:', layout=widgets.Layout(width='60%'))\n",
    "    btn_run = widgets.Button(description='â–¶ Run Simulation', button_style='warning', icon='cogs')\n",
    "    out_log = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with out_log:\n",
    "            clear_output()\n",
    "            selected_video = VIDEO_DIR / dropdown.value\n",
    "            new_name = f\"{selected_video.stem}_async_sim.mp4\"\n",
    "            print(f\"ðŸš€ Simulating Onboard Computer for: {selected_video.name}...\")\n",
    "            run_async_simulation(\n",
    "                video_path=selected_video,\n",
    "                model=model,\n",
    "                processor=processor,\n",
    "                tmpl=tmpl,\n",
    "                out_path=Path(new_name),\n",
    "                reasoning_interval=10 # Update text every 10 frames\n",
    "            )\n",
    "\n",
    "    btn_run.on_click(on_button_click)\n",
    "    display(widgets.VBox([dropdown, btn_run, out_log]))\n",
    "\n",
    "launch_async_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702e616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
