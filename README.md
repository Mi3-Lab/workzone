# ğŸš§ WorkZone Detection System

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![YOLOv12](https://img.shields.io/badge/YOLO-v12-00FFFF.svg)](https://github.com/ultralytics/ultralytics)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Real-time construction work zone detection and monitoring system using state-of-the-art computer vision.**

Built for ESV (Enhanced Safety of Vehicles) competition. Features multi-modal verification (YOLO + CLIP + OCR), temporal attention, scene context classification, and edge deployment optimization for NVIDIA Jetson Orin.

---

## âš¡ Key Features

| Feature | Description |
|---------|-------------|
| **ğŸ¯ YOLO12s Detection** | 50-class object detection with 84.6% false positive reduction |
| **ğŸ§  Multi-Modal Fusion** | CLIP semantic verification + OCR text extraction |
| **ğŸ“Š Temporal Attention** | Phase 2.1: Per-cue confidence tracking + motion plausibility |
| **ğŸŒ Scene Context** | Highway/Urban/Suburban classification (92.8% accuracy) |
| **ğŸ”„ Adaptive State Machine** | Context-aware thresholds: OUT â†’ APPROACHING â†’ INSIDE â†’ EXITING |
| **âš¡ Edge Optimized** | Runs 30 FPS @ 1280px on Jetson Orin |
| **ğŸ¬ Interactive UI** | Streamlit calibration app with real-time visualization |

---

## ğŸ¯ Performance Highlights

| Component | Metric | Value |
|-----------|--------|-------|
| **YOLO Detection** | False Positive Reduction | **84.6%** vs baseline |
| **YOLO Detection** | Inference Speed (A100) | **~85 FPS** @ 1280px |
| **YOLO Detection** | Inference Speed (Jetson) | **~30 FPS** @ 1280px |
| **Scene Context** | Classification Accuracy | **92.8%** |
| **OCR Classification** | Test Accuracy | **97.7%** (43/44) |
| **System** | GPU Memory (batch=1) | **2.4 GB** |

---

## ğŸ“¦ Installation

### Prerequisites

- **Python**: 3.10 or 3.11 (3.12 not tested)
- **OS**: Linux (Ubuntu 20.04+), macOS, Windows
- **GPU**: NVIDIA GPU with CUDA 11.8+ (recommended) or CPU
- **RAM**: 16GB minimum, 32GB recommended
- **Disk**: ~10GB for models + data

### Step 1: Clone Repository

```bash
git clone https://github.com/WMaia9/workzone.git
cd workzone
```

### Step 2: Create Virtual Environment

```bash
# Create venv
python3.11 -m venv .venv

# Activate (Linux/macOS)
source .venv/bin/activate

# Activate (Windows)
.venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
# Install core dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Install workzone package
pip install -e .
```

**Note**: For **CPU-only** installation, install PyTorch CPU version first:
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
pip install -e .
```

### Step 4: Download Pre-trained Models

```bash
# Download all required models (~3GB)
bash scripts/download_models.sh
```

This downloads:
- âœ… `yolo12s_hardneg_1280.pt` - Hard-negative trained YOLO (recommended)
- âœ… `yolo12s_fusion_baseline.pt` - Baseline YOLO model
- âœ… `scene_context_classifier.pt` - Phase 1.4 scene context model
- âœ… CLIP ViT-B/32 (auto-downloaded on first run)

### Step 5: Verify Installation

```bash
# Quick test - process demo video
python scripts/process_video_fusion.py \
  data/demo/boston_workzone_short.mp4 \
  --output-dir outputs/test \
  --stride 5

# Expected: Annotated video + CSV timeline in outputs/test/
```

---

## ğŸš€ Quick Start

### Option 1: Interactive Calibration App (Recommended)

Launch the **Streamlit calibration UI** for interactive parameter tuning:

```bash
streamlit run src/workzone/apps/streamlit/app_phase2_1_evaluation.py
```

**Features**:
- ğŸ“¹ Real-time video preview with live parameter adjustment
- ğŸ“Š Batch processing with explainability dashboards
- ğŸ’¾ Export annotated videos + detailed CSV timelines
- ğŸšï¸ Calibrate YOLO weights, CLIP fusion, OCR boost, state machine
- ğŸ”¬ Phase 2.1: Per-cue confidences + motion plausibility visualizations

ğŸ‘‰ **See [APP_TESTING_GUIDE.md](APP_TESTING_GUIDE.md)** for detailed usage instructions.

---

### Option 2: Command-Line Batch Processing

Process videos from the command line for high-throughput workflows:

#### Basic Usage

```bash
python scripts/process_video_fusion.py \
  path/to/video.mp4 \
  --output-dir outputs/my_run
```

#### Phase 1.1: Multi-Cue Temporal Persistence

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/phase1_1 \
  --enable-phase1-1 \
  --no-motion
```

#### Phase 1.4: Scene Context Classification

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/phase1_4 \
  --enable-phase1-4 \
  --enable-ocr
```

#### Phase 2.1: Per-Cue Verification + Motion Tracking

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/phase2_1 \
  --enable-phase2-1 \
  --enable-phase1-1 \
  --enable-ocr \
  --no-motion \
  --stride 2
```

#### Full Pipeline (All Features)

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/full \
  --enable-phase1-1 \
  --enable-phase1-4 \
  --enable-phase2-1 \
  --enable-ocr \
  --device cuda \
  --stride 2 \
  --clip-weight 0.35 \
  --clip-trigger-th 0.45 \
  --enter-th 0.70 \
  --exit-th 0.45
```

#### Common Options

| Option | Description | Default |
|--------|-------------|---------|
| `--device` | Device: `cuda` or `cpu` | `cuda` |
| `--conf` | YOLO confidence threshold | `0.25` |
| `--stride` | Frame stride (1 = every frame) | `2` |
| `--enable-ocr` | Enable OCR text extraction | `False` |
| `--enable-phase1-1` | Multi-cue temporal logic | `False` |
| `--enable-phase1-4` | Scene context classification | `False` |
| `--enable-phase2-1` | Per-cue CLIP + motion tracking | `False` |
| `--clip-weight` | CLIP fusion weight | `0.35` |
| `--clip-trigger-th` | CLIP trigger threshold | `0.45` |
| `--enter-th` | WORKZONE entry threshold | `0.70` |
| `--exit-th` | WORKZONE exit threshold | `0.45` |
| `--no-video` | Skip video output (faster) | `False` |
| `--no-csv` | Skip CSV output | `False` |

---

## ğŸ“Š System Architecture

### Detection Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Video    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLO12s Object Detection                       â”‚
â”‚  â€¢ 50 work zone classes                         â”‚
â”‚  â€¢ Hard-negative trained (84.6% FP reduction)   â”‚
â”‚  â€¢ 1280px @ 30 FPS (Jetson Orin)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                  â–¼                  â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLIP Semantic  â”‚  â”‚ OCR Text        â”‚  â”‚ Scene Contextâ”‚  â”‚ Per-Cue CLIP    â”‚
â”‚ Verification   â”‚  â”‚ Extraction      â”‚  â”‚ Classifier   â”‚  â”‚ (Phase 2.1)     â”‚
â”‚ (Global)       â”‚  â”‚ (Message Boards)â”‚  â”‚ (Phase 1.4)  â”‚  â”‚ â€¢ Channelizationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ Workers       â”‚
         â”‚                   â”‚                   â”‚          â”‚ â€¢ Vehicles      â”‚
         â”‚                   â”‚                   â”‚          â”‚ â€¢ Signs         â”‚
         â”‚                   â”‚                   â”‚          â”‚ â€¢ Equipment     â”‚
         â”‚                   â”‚                   â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Multi-Modal Fusion   â”‚
                          â”‚  â€¢ Weighted EMA       â”‚
                          â”‚  â€¢ Context Boost      â”‚
                          â”‚  â€¢ OCR Boost          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  State Machine        â”‚
                          â”‚  OUT â†’ APPROACHING    â”‚
                          â”‚      â†’ INSIDE         â”‚
                          â”‚      â†’ EXITING â†’ OUT  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Output               â”‚
                          â”‚  â€¢ Annotated Video    â”‚
                          â”‚  â€¢ Timeline CSV       â”‚
                          â”‚  â€¢ State Transitions  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase Progression

| Phase | Feature | Description |
|-------|---------|-------------|
| **1.0** | Base System | YOLO + CLIP + EMA + State Machine |
| **1.1** | Multi-Cue Logic | Temporal persistence tracking (5 cue types) |
| **1.2** | Hard-Negative Mining | 84.6% FP reduction through iterative training |
| **1.3** | Motion Validation | Trajectory-based false positive filtering |
| **1.4** | Scene Context | Highway/Urban/Suburban classification (92.8%) |
| **2.1** | Per-Cue Verification | CLIP confidence per cue + motion plausibility |

---

## ğŸ“‚ Repository Structure

```
workzone/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ APP_TESTING_GUIDE.md              # Comprehensive calibration guide
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pyproject.toml                    # Package configuration
â”œâ”€â”€ setup.py                          # Installation script
â”‚
â”œâ”€â”€ configs/                          # Configuration files
â”‚   â”œâ”€â”€ config.yaml                   # Main config
â”‚   â”œâ”€â”€ multi_cue_config.yaml         # Phase 1.1 multi-cue settings
â”‚   â””â”€â”€ motion_cue_config.yaml        # Phase 1.3 motion settings
â”‚
â”œâ”€â”€ data/                             # Data directory (gitignored)
â”‚   â”œâ”€â”€ 01_raw/                       # Raw videos
â”‚   â”œâ”€â”€ 02_processed/                 # Processed annotations
â”‚   â”œâ”€â”€ 03_demo/                      # Demo videos
â”‚   â”œâ”€â”€ 04_derivatives/               # Hard-negative mining outputs
â”‚   â””â”€â”€ 05_workzone_yolo/             # YOLO training data
â”‚
â”œâ”€â”€ weights/                          # Pre-trained models (download via script)
â”‚   â”œâ”€â”€ yolo12s_hardneg_1280.pt      # Recommended model
â”‚   â”œâ”€â”€ yolo12s_fusion_baseline.pt   # Baseline model
â”‚   â”œâ”€â”€ scene_context_classifier.pt  # Phase 1.4 model
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                          # CLI tools
â”‚   â”œâ”€â”€ process_video_fusion.py       # Main video processing CLI
â”‚   â”œâ”€â”€ download_models.sh            # Model download script
â”‚   â”œâ”€â”€ mine_hard_negatives.py        # Hard-negative mining
â”‚   â”œâ”€â”€ train_scene_context.py        # Scene context training
â”‚   â””â”€â”€ evaluate_phase1_4.py          # Phase 1.4 evaluation
â”‚
â”œâ”€â”€ src/workzone/                     # Core package
â”‚   â”œâ”€â”€ detection/                    # Detection components
â”‚   â”‚   â”œâ”€â”€ yolo_detector.py          # YOLO wrapper
â”‚   â”‚   â””â”€â”€ cue_classifier.py         # Multi-cue classification
â”‚   â”œâ”€â”€ fusion/                       # Multi-modal fusion
â”‚   â”‚   â”œâ”€â”€ clip_verifier.py          # CLIP semantic verification
â”‚   â”‚   â””â”€â”€ multi_cue_gate.py         # Phase 1.1 AND gate
â”‚   â”œâ”€â”€ ocr/                          # OCR text extraction
â”‚   â”‚   â”œâ”€â”€ text_detector.py          # EasyOCR/Paddle wrapper
â”‚   â”‚   â””â”€â”€ text_classifier.py        # Text category classification
â”‚   â”œâ”€â”€ models/                       # Advanced models
â”‚   â”‚   â”œâ”€â”€ scene_context.py          # Phase 1.4 scene classifier
â”‚   â”‚   â”œâ”€â”€ per_cue_verification.py   # Phase 2.1 per-cue CLIP
â”‚   â”‚   â””â”€â”€ trajectory_tracking.py    # Phase 2.1 motion plausibility
â”‚   â”œâ”€â”€ temporal/                     # Temporal logic
â”‚   â”‚   â””â”€â”€ persistence_tracker.py    # Phase 1.1 persistence
â”‚   â”œâ”€â”€ state/                        # State machine
â”‚   â”‚   â””â”€â”€ workzone_states.py        # State transitions
â”‚   â””â”€â”€ apps/                         # Applications
â”‚       â””â”€â”€ streamlit/                # Streamlit UI
â”‚           â””â”€â”€ app_phase2_1_evaluation.py  # Calibration app
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_workzone_yolo_setup.ipynb
â”‚   â”œâ”€â”€ 02_workzone_yolo_train_eval.ipynb
â”‚   â”œâ”€â”€ 03_workzone_yolo_video_demo.ipynb
â”‚   â”œâ”€â”€ 04_workzone_video_state_machine.ipynb
â”‚   â”œâ”€â”€ 05_workzone_video_timeline_calibration.ipynb
â”‚   â”œâ”€â”€ 06_triggered_vlm_semantic_verification.ipynb
â”‚   â””â”€â”€ 07_phase1_4_scene_context.ipynb
â”‚
â”œâ”€â”€ tests/                            # Unit tests
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_pipelines.py
â”‚
â”œâ”€â”€ docs/                             # Documentation
â”‚   â”œâ”€â”€ MODEL_REGISTRY.md             # Model performance metrics
â”‚   â”œâ”€â”€ PHASE1_3.md                   # Phase 1.3 motion validation
â”‚   â””â”€â”€ guides/                       # User guides
â”‚
â””â”€â”€ outputs/                          # Processing outputs (gitignored)
    â”œâ”€â”€ phase1_1_demo/
    â”œâ”€â”€ phase1_4_demo/
    â”œâ”€â”€ phase2_1_demo/
    â””â”€â”€ ...
```

---

## ğŸ”¬ Advanced Usage

### Training Custom Models

#### YOLO Fine-tuning

```bash
cd workzone-yolo-v12/
yolo train \
  data=workzone.yaml \
  model=yolo12s.pt \
  epochs=50 \
  imgsz=1280 \
  batch=8 \
  device=0
```

#### Scene Context Training

```bash
python scripts/train_scene_context.py \
  --data-root data/05_workzone_yolo \
  --output-dir runs/scene_context \
  --epochs 30 \
  --batch-size 32 \
  --backbone resnet18
```

#### Phase 2.1 Temporal Attention Training

```bash
python scripts/train_phase2_1_attention.py \
  --data-path data/phase2_1_trajectories \
  --output-dir runs/phase2_1_attention \
  --epochs 50 \
  --batch-size 16 \
  --device cuda
```

### Hard-Negative Mining

See [docs/reports/PHASE1_2_MINING_REPORT.md](docs/reports/PHASE1_2_MINING_REPORT.md) for details.

```bash
# 1. Mine candidates from video dataset
bash scripts/HARDNEG_QUICKSTART.sh

# 2. Review and categorize
python scripts/review_hard_negatives.py

# 3. Consolidate annotations
python scripts/consolidate_candidates.py

# 4. Retrain YOLO
cd workzone-yolo-v12/
yolo train data=workzone_hardneg.yaml model=yolo12s.pt ...
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=workzone --cov-report=html

# Run specific test
pytest tests/test_models.py -v
```

---

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| [APP_TESTING_GUIDE.md](APP_TESTING_GUIDE.md) | **Comprehensive calibration guide** with all parameters explained |
| [docs/MODEL_REGISTRY.md](docs/MODEL_REGISTRY.md) | Model performance benchmarks |
| [docs/PHASE1_3.md](docs/PHASE1_3.md) | Motion validation details |
| [docs/reports/PHASE1_2_MINING_REPORT.md](docs/reports/PHASE1_2_MINING_REPORT.md) | Hard-negative mining methodology |

---

## ğŸ¤ Contributing

Contributions welcome! Please see [CONTRIBUTING.md](alpamayo/CONTRIBUTING.md) for guidelines.

---

## ğŸ“„ License

MIT License - see [LICENSE](alpamayo/LICENSE) for details.

---

## ğŸ™ Acknowledgments

- ESV Competition organizers
- Ultralytics for YOLOv12
- OpenAI for CLIP
- PaddleOCR and EasyOCR teams
- W&B for experiment tracking

---

## ğŸ“§ Contact

For questions or feedback:
- **GitHub Issues**: [github.com/WMaia9/workzone/issues](https://github.com/WMaia9/workzone/issues)
- **Email**: [your-email@domain.com]

---

**Built with â¤ï¸ for safer roads**
