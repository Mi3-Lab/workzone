# WorkZone: AI-Powered Construction Zone Detection

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Real-time construction zone detection using YOLO with semantic verification via CLIP. Built for the ESV (Enhanced Safety of Vehicles) competition and optimized for deployment on edge devices (Jetson Orin).

## ‚ö° Quick Features

- üöó **Real-time YOLO detection** - 50-class construction zone object detection
- üé® **Lightweight fusion & smoothing** - Adaptive EMA + orange-cue context boost
- üìä **State machine tracking** - Anti-flicker work zone states (OUT ‚Üí APPROACHING ‚Üí INSIDE ‚Üí EXITING)
- üé¨ **Interactive apps** - Streamlit web UIs with live preview and batch processing
- üíª **Edge-ready** - Optimized for Jetson Orin; FP16 inference, configurable stride
- üìà **Experiment tracking** - Weights & Biases integration

## üöÄ Installation

```bash
# Clone repository
git clone https://github.com/WMaia9/workzone.git
cd workzone

# System dependencies (Linux)
sudo apt update
sudo apt install -y libgl1 libglib2.0-0 libsm6 libxext6 ffmpeg

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip setuptools wheel

# Install requirements
pip install -r requirements.txt

# Install package in development mode
pip install -e .
```

## üìñ Usage

### Training YOLO

```bash
python -m src.workzone.cli.train_yolo \
  --model yolo12s.pt \
  --epochs 300 \
  --batch 32 \
  --device 0
```

### Video Inference

```bash
python -m src.workzone.cli.infer_video \
  --video data/demo/sample.mp4 \
  --model weights/best.pt \
  --output result.mp4
```

### Python API

```python
from src.workzone.models.yolo_detector import YOLODetector
from src.workzone.pipelines.video_inference import VideoInferencePipeline
from pathlib import Path

detector = YOLODetector("weights/best.pt", device="cuda")
pipeline = VideoInferencePipeline(detector, Path("video.mp4"), Path("output.mp4"))
results = pipeline.process()
```

## üé® Web Applications

Three production-ready Streamlit apps for interactive analysis:

```bash
# Activate environment
source venv/bin/activate

# Basic YOLO detection with simple scoring
streamlit run src/workzone/apps/streamlit/app_basic_detection.py

# Advanced semantic scoring (z-scores, EMA smoothing)
streamlit run src/workzone/apps/streamlit/app_advanced_scoring.py

# YOLO + CLIP fusion with adaptive EMA and context boost (RECOMMENDED)
streamlit run src/workzone/apps/streamlit/app_semantic_fusion.py
```

**Features**:
- **Live preview** (real-time playback with instant feedback)
- **Batch processing** (save outputs: video + CSV timeline)
- **Score visualization** (YOLO and fused score curves)
- **Device selection** (Auto-detect GPU, manual CPU/CUDA)
- **Fusion boosts**:
  - ‚úÖ Adaptive EMA smoothing (scales from 0.4√ó to 1.2√ó base alpha based on evidence)
  - ‚úÖ Orange-cue context boost (HSV-based work zone color detection when YOLO is uncertain)
  - ‚úÖ Tunable HSV thresholds for traffic cone/barrier colors
- **CSV export** (frame-by-frame scores, states, clip usage)
- **Model upload** (test with custom YOLO weights)

### Streamlit Cloud Deployment

- Place `packages.txt` (apt dependencies) alongside your app. This repo stores it at `src/workzone/apps/streamlit/packages.txt`.
- Example apt packages: `libgl1`, `libglib2.0-0`, `ffmpeg`.

### Vision-Language Apps (Optional)

#### Alpamayo Setup

```bash
# Clone NVLabs Alpamayo (once)
git clone https://github.com/NVlabs/alpamayo.git

# Activate environment
source venv/bin/activate

# Install flash-attn (compatible wheels)
pip install flash-attn --no-build-isolation

# Install Alpamayo package (will install compatible Torch/Transformers)
pip install -e ./alpamayo
```

#### Run Inspectors

```bash
# 10Hz VLA reasoning with overlay (press 'q' to quit)
python -m src.workzone.apps.alpamayo.alpamayo_10hz_inspector \
  --video data/demo/boston_workzone_short.mp4 \
  --output alpamayo_output.mp4

# Zero-lag threaded VLA player
python -m src.workzone.apps.alpamayo.alpamayo_threaded \
  --video data/demo/boston_workzone_short.mp4
```

#### Quick Run (Short Commands)

After `pip install -e .`, you can use short CLI commands:

```bash
# 10Hz inspector
workzone-alpamayo-inspector --video data/demo/boston_workzone_short.mp4 --output alpamayo_output.mp4

# Zero-lag threaded player
workzone-alpamayo-threaded --video data/demo/boston_workzone_short.mp4
```

## üìÅ Project Structure

```
src/workzone/
‚îú‚îÄ‚îÄ models/              # YOLO, CLIP, Alpamayo wrappers
‚îú‚îÄ‚îÄ pipelines/          # Training and inference pipelines
‚îú‚îÄ‚îÄ cli/                # Command-line interfaces
‚îú‚îÄ‚îÄ apps/              # Streamlit and Alpamayo apps
‚îÇ   ‚îú‚îÄ‚îÄ streamlit/     # Web UIs
‚îÇ   ‚îî‚îÄ‚îÄ alpamayo/      # VLA apps
‚îú‚îÄ‚îÄ config.py          # Configuration management
‚îî‚îÄ‚îÄ utils/             # Logging, path utilities
```

## ‚öôÔ∏è Configuration

YAML-based configuration:

```bash
# Edit configs/config.yaml
yolo:
  model_name: yolo12s
  imgsz: 960
  batch_size: 32
  device: cuda:0
```

Or use environment variables:

```bash
export WORKZONE_YOLO_MODEL=yolo12s
export WORKZONE_YOLO_DEVICE=cuda:0
```

Or Python API:

```python
from src.workzone.config import ProjectConfig, YOLOConfig

config = ProjectConfig(
    device="cuda",
    yolo=YOLOConfig(model_name="yolo12s", imgsz=960, epochs=300)
)
```

## üß™ Testing

```bash
# Run tests
pytest

# With coverage
pytest --cov=src/workzone

# Format code
black src/ tests/
isort src/ tests/

# Lint & type check
flake8 src/ tests/
mypy src/
```

## üìä Performance

| Model | Size | FPS (batch=1) | GPU Memory | Notes |
|-------|------|---------------|-----------|-------|
| YOLOv12s | 960√ó960 | ~85 | 2.4GB | RTX 4090, FP16 |
| YOLOv12s | 960√ó960 (stride=2) | ~150 | 2.4GB | Real-time on Orin estimate |
| YOLOv8s | 640√ó640 (batch=32) | ~240 | 12GB | Batch mode, A100 |

**Latency (single frame, GPU)**:
- YOLO inference: ~12ms (960√ó960, FP16)
- Orange cue: <1ms
- CLIP (optional, triggered): ~30ms
- Total: 12‚Äì42ms depending on trigger

**Fusion model**: Adaptive EMA + lightweight context boost (no additional latency vs. base YOLO)

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open a Pull Request

**Code guidelines**:
- PEP 8 compliance with black formatter
- Type hints on all functions
- Docstrings for classes/methods
- Tests for new functionality

## üìù License

MIT License - see [LICENSE](LICENSE) file

## üôè Acknowledgments

- **Ultralytics** - YOLOv8/v11/v12
- **OpenAI** - CLIP model for semantic verification
- **Weights & Biases** - Experiment tracking
- **ESV Competition** - Competition organizers
- **Streamlit** - Interactive web applications

---

**Built for the ESV Competition** üèÜ | **Edge-ready for Jetson Orin** üöÄ
