# üöß WorkZone Detection System

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![YOLOv12](https://img.shields.io/badge/YOLO-v12-00FFFF.svg)](https://github.com/ultralytics/ultralytics)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Real-time construction work zone detection and monitoring system using state-of-the-art computer vision.**

Built for ESV (Enhanced Safety of Vehicles) competition. Features multi-modal verification (YOLO + CLIP + OCR), temporal attention, scene context classification, and edge deployment optimization for NVIDIA Jetson Orin.

---

## ‚ö° Key Features

| Feature | Description |
|---------|-------------|
| **üéØ YOLO12s Detection** | 50-class object detection with 84.6% false positive reduction |
| **üß† Multi-Modal Fusion** | CLIP semantic verification + OCR text extraction |
| **ÔøΩ OCR Speed Sign Detection** | Priority-based OCR with visual alerts for speed limit signs |
| **üìä Per-Cue Verification** | Per-cue confidence tracking + motion plausibility |
| **üåç Scene Context** | Highway/Urban/Suburban classification (92.8% accuracy) |
| **üîÑ Adaptive State Machine** | Context-aware thresholds: OUT ‚Üí APPROACHING ‚Üí INSIDE ‚Üí EXITING |
| **‚ö° Edge Optimized** | Runs 30 FPS @ 1280px on Jetson Orin |
| **üé¨ Interactive UI** | Streamlit calibration app with video player and real-time tuning |

---

## üéØ Performance Highlights

| Component | Metric | Value |
|-----------|--------|-------|
| **YOLO Detection** | False Positive Reduction | **84.6%** vs baseline |
| **YOLO Detection** | Inference Speed (A100) | **~85 FPS** @ 1280px |
| **YOLO Detection** | Inference Speed (Jetson) | **~30 FPS** @ 1280px |
| **Scene Context** | Classification Accuracy | **92.8%** |
| **OCR Classification** | Test Accuracy | **97.7%** (43/44) |
| **System** | GPU Memory (batch=1) | **2.4 GB** |

---

## üì¶ Installation

### Prerequisites

- **Python**: 3.10 or 3.11 (3.12 not tested)
- **OS**: Linux (Ubuntu 20.04+), macOS, Windows
- **GPU**: NVIDIA GPU with CUDA 11.8+ (recommended) or CPU
- **RAM**: 16GB minimum, 32GB recommended
- **Disk**: ~10GB for models + data

### Step 1: Clone Repository

```bash
git clone https://github.com/WMaia9/workzone.git
cd workzone
```

### Step 2: Create Virtual Environment

```bash
# Create venv
python -m venv venv

# Activate (Linux/macOS)
source venv/bin/activate

# Activate (Windows)
venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
# Install core dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Install workzone package
pip install -e .
```

**Note**: For **CPU-only** installation, install PyTorch CPU version first:
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
pip install -e .
```

### NVIDIA Jetson Installation (Orin / Nano - JetPack 6.2)

For deployment on NVIDIA Jetson devices running **JetPack 6.2 (L4T 36.4.7)** with **CUDA 12.6**, follow these specialized steps to ensure GPU/TensorRT support:

#### 1. Install Specialized PyTorch Wheels
Standard PyPI wheels do not support Jetson GPU acceleration. Use the optimized wheels for JetPack 6:

```bash
# Recommended: PyTorch 2.8.0 + Torchvision 0.23.0
wget https://github.com/davidl-nv/torch/raw/main/torch-2.8/torch-2.8.0-cp310-cp310-linux_aarch64.whl
wget https://github.com/davidl-nv/torch/raw/main/torch-2.8/torchvision-0.23.0-cp310-cp310-linux_aarch64.whl

source venv/bin/activate
pip install torch-2.8.0-cp310-cp310-linux_aarch64.whl torchvision-0.23.0-cp310-cp310-linux_aarch64.whl
```

#### 2. Fix System Dependencies
If you encounter `ImportError: libcusparseLt.so.0`, manually install the missing library:

```bash
wget https://developer.download.nvidia.com/compute/cusparselt/redist/libcusparse_lt/linux-aarch64/libcusparse_lt-linux-aarch64-0.6.2.3-archive.tar.xz
tar -xf libcusparse_lt-linux-aarch64-0.6.2.3-archive.tar.xz
export LD_LIBRARY_PATH=$(pwd)/libcusparse_lt-linux-aarch64-0.6.2.3-archive/lib:$LD_LIBRARY_PATH
```

#### 3. Resolve NumPy ABI Conflicts
Jetson PyTorch wheels are often compiled against NumPy 1.x. Ensure compatibility:

```bash
pip install "numpy<2"
```

### Step 4: Verify Installation

```bash
# Verify all dependencies are correctly installed
scripts/verify_installation.sh
```

This script checks:
- ‚úÖ Python virtual environment
- ‚úÖ All required Python packages
- ‚úÖ WorkZone package installation
- ‚úÖ Model weights availability

### Step 5: Quick Test

```bash
# Quick test - process demo video
python scripts/process_video_fusion.py \
  data/demo/boston_workzone_short.mp4 \
  --output-dir outputs/test \
  --stride 5

# Expected: Annotated video + CSV timeline in outputs/test/
```

### Troubleshooting

**Common Issues:**

- **"ModuleNotFoundError: No module named 'workzone'"**
  ```bash
  # Install the package
  pip install -e .
  ```

- **CUDA/PyTorch issues**
  ```bash
  # For CPU-only installation
  pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
  ```

- **Permission issues**
  ```bash
  # Make scripts executable
  chmod +x scripts/setup.sh scripts/launch_streamlit.sh scripts/verify_installation.sh
  ```

- **Model loading issues**
  ```bash
  # Check model files
  ls -la weights/
  ```

---

## üöÄ Quick Start

### Run the App (Professional Way)

```bash
# Quick launch
make app
# or
make streamlit
```

The app will be available at `http://localhost:8502`

### Option 1: Interactive Calibration App (Recommended)

The **Streamlit calibration UI** provides interactive parameter tuning:

**Alternative manual launch**:
```bash
source venv/bin/activate
streamlit run src/workzone/apps/streamlit/app_phase2_1_evaluation.py
# or
scripts/launch_streamlit.sh
```

**Features**:
- üìπ Real-time video preview with live parameter adjustment
- üìä Batch processing with comprehensive explainability dashboards
- üé¨ Built-in video player with play/pause/seek controls (batch mode)
- üíæ Export annotated videos (H.264) + detailed CSV timelines
- üèõÔ∏è Calibrate YOLO weights, CLIP fusion, OCR boost, state machine thresholds
- üìù OCR customization: confidence threshold, score boost, speed sign priority
- üî¨ Per-cue confidences + motion plausibility tracking
- ‚ö° Component throughput visualization (Hz, ms/frame)
- üìà Advanced analysis: score zones, state distribution, latency profiling

üëâ **See [APP_TESTING_GUIDE.md](docs/guides/APP_TESTING_GUIDE.md)** for detailed usage instructions.

#### Backend Selection (Auto/TensorRT/GPU/CPU)

In the Streamlit sidebar, choose your inference backend under "Model + Device":

- Auto (prefer TensorRT): uses `.engine` if present; else CUDA; else CPU
- TensorRT: forces `.engine` (Tensor Cores); falls back to `.pt` if load fails
- GPU (cuda): forces `.pt` on CUDA even if `.engine` exists (useful for comparisons)
- CPU: forces `.pt` on CPU for portability

Expected logs:

```
üöÄ TensorRT engine found: yolo12s_hardneg_1280.engine
‚úì Loaded TensorRT model (optimized for Tensor Cores)

Loaded YOLO from yolo12s_hardneg_1280.pt (FP16/FP32)
```

#### Convert YOLO to TensorRT (optional, for maximum speed)

```bash
source venv/bin/activate
# Convert a specific model to .engine (FP16)
python scripts/optimize_for_jetson.py --model weights/yolo12s_hardneg_1280.pt

# Or convert all .pt models in weights/
python scripts/optimize_for_jetson.py
```

After conversion, run Streamlit (Auto/TensorRT will pick the `.engine` automatically).

For Jetson-specific tips and deployment steps, see [JETSON_OPTIMIZATION.md](docs/JETSON_OPTIMIZATION.md).

---

### Option 2: Command-Line Batch Processing

Process videos from the command line for high-throughput workflows:

#### Basic Usage

```bash
python scripts/process_video_fusion.py \
  path/to/video.mp4 \
  --output-dir outputs/my_run
```

#### Multi-Cue Temporal Detection

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/multi_cue \
  --enable-phase1-1 \
  --no-motion
```

#### Scene Context Classification

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/scene_context \
  --enable-phase1-4 \
  --enable-ocr
```

#### Per-Cue Verification + Motion Tracking

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/per_cue_motion \
  --enable-phase2-1 \
  --enable-phase1-1 \
  --enable-ocr \
  --no-motion \
  --stride 2
```

#### Full Pipeline (All Features)

```bash
python scripts/process_video_fusion.py \
  data/demo/video.mp4 \
  --output-dir outputs/full \
  --enable-phase1-1 \
  --enable-phase1-4 \
  --enable-phase2-1 \
  --enable-ocr \
  --device cuda \
  --stride 2 \
  --clip-weight 0.35 \
  --clip-trigger-th 0.45 \
  --enter-th 0.70 \
  --exit-th 0.45
```

#### Common Options

| Option | Description | Default |
|--------|-------------|---------|
| `--device` | Device: `cuda` or `cpu` | `cuda` |
| `--conf` | YOLO confidence threshold | `0.25` |
| `--stride` | Frame stride (1 = every frame) | `2` |
| `--enable-ocr` | Enable OCR text extraction | `False` |
| `--enable-phase1-1` | Multi-cue temporal logic | `False` |
| `--enable-phase1-4` | Scene context classification | `False` |
| `--enable-phase2-1` | Per-cue CLIP + motion tracking | `False` |

**Note**: Flag names preserve `phase` for backward compatibility but represent unified system components.
| `--clip-weight` | CLIP fusion weight | `0.35` |
| `--clip-trigger-th` | CLIP trigger threshold | `0.45` |
| `--enter-th` | WORKZONE entry threshold | `0.70` |
| `--exit-th` | WORKZONE exit threshold | `0.45` |
| `--no-video` | Skip video output (faster) | `False` |
| `--no-csv` | Skip CSV output | `False` |

---

## üìä System Architecture

### Detection Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Input Video    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  YOLO12s Object Detection                       ‚îÇ
‚îÇ  ‚Ä¢ 50 work zone classes                         ‚îÇ
‚îÇ  ‚Ä¢ Hard-negative trained (84.6% FP reduction)   ‚îÇ
‚îÇ  ‚Ä¢ 1280px @ 30 FPS (Jetson Orin)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº                  ‚ñº                  ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CLIP Semantic  ‚îÇ  ‚îÇ OCR Text        ‚îÇ  ‚îÇ Scene Context‚îÇ  ‚îÇ Per-Cue CLIP    ‚îÇ
‚îÇ Verification   ‚îÇ  ‚îÇ Extraction      ‚îÇ  ‚îÇ Classifier   ‚îÇ  ‚îÇ Verification    ‚îÇ
‚îÇ (Global)       ‚îÇ  ‚îÇ (Message Boards)‚îÇ  ‚îÇ (Highway/    ‚îÇ  ‚îÇ ‚Ä¢ Channelization‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  Urban/      ‚îÇ  ‚îÇ ‚Ä¢ Workers       ‚îÇ
         ‚îÇ                   ‚îÇ           ‚îÇ  Suburban)   ‚îÇ  ‚îÇ ‚Ä¢ Vehicles      ‚îÇ
         ‚îÇ                   ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚Ä¢ Signs         ‚îÇ
         ‚îÇ                   ‚îÇ                  ‚îÇ          ‚îÇ ‚Ä¢ Equipment     ‚îÇ
         ‚îÇ                   ‚îÇ                  ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                   ‚îÇ                  ‚îÇ                   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ  Multi-Modal Fusion   ‚îÇ
                          ‚îÇ  ‚Ä¢ Weighted EMA       ‚îÇ
                          ‚îÇ  ‚Ä¢ Context Boost      ‚îÇ
                          ‚îÇ  ‚Ä¢ OCR Boost          ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ  State Machine        ‚îÇ
                          ‚îÇ  OUT ‚Üí APPROACHING    ‚îÇ
                          ‚îÇ      ‚Üí INSIDE         ‚îÇ
                          ‚îÇ      ‚Üí EXITING ‚Üí OUT  ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ  Output               ‚îÇ
                          ‚îÇ  ‚Ä¢ Annotated Video    ‚îÇ
                          ‚îÇ  ‚Ä¢ Timeline CSV       ‚îÇ
                          ‚îÇ  ‚Ä¢ State Transitions  ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Phase Progression

| Phase | Feature | Description |
|-------|---------|-------------|
| **1.0** | Base System | YOLO + CLIP + EMA + State Machine |
| **1.1** | Multi-Cue Logic | Temporal persistence tracking (5 cue types) |
| **1.2** | Hard-Negative Mining | 84.6% FP reduction through iterative training |
| **1.3** | Motion Validation | Trajectory-based false positive filtering |
| **1.4** | Scene Context | Highway/Urban/Suburban classification (92.8%) |
| **2.1** | Per-Cue Verification | CLIP confidence per cue + motion plausibility |

---

## üìÇ Repository Structure

```
workzone/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ APP_TESTING_GUIDE.md              # Comprehensive calibration guide
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ pyproject.toml                    # Package configuration
‚îú‚îÄ‚îÄ setup.py                          # Installation script
‚îÇ
‚îú‚îÄ‚îÄ configs/                          # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml                   # Main config
‚îÇ   ‚îú‚îÄ‚îÄ multi_cue_config.yaml         # Phase 1.1 multi-cue settings
‚îÇ   ‚îî‚îÄ‚îÄ motion_cue_config.yaml        # Phase 1.3 motion settings
‚îÇ
‚îú‚îÄ‚îÄ data/                             # Data directory (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ 01_raw/                       # Raw videos
‚îÇ   ‚îú‚îÄ‚îÄ 02_processed/                 # Processed annotations
‚îÇ   ‚îú‚îÄ‚îÄ 03_demo/                      # Demo videos
‚îÇ   ‚îú‚îÄ‚îÄ 04_derivatives/               # Hard-negative mining outputs
‚îÇ   ‚îî‚îÄ‚îÄ 05_workzone_yolo/             # YOLO training data
‚îÇ
‚îú‚îÄ‚îÄ weights/                          # Pre-trained models (download via script)
‚îÇ   ‚îú‚îÄ‚îÄ yolo12s_hardneg_1280.pt      # Recommended model
‚îÇ   ‚îú‚îÄ‚îÄ yolo12s_fusion_baseline.pt   # Baseline model
‚îÇ   ‚îú‚îÄ‚îÄ scene_context_classifier.pt  # Phase 1.4 model
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ scripts/                          # CLI tools
‚îÇ   ‚îú‚îÄ‚îÄ process_video_fusion.py       # Main video processing CLI
‚îÇ   ‚îú‚îÄ‚îÄ download_models.sh            # Model download script
‚îÇ   ‚îú‚îÄ‚îÄ mine_hard_negatives.py        # Hard-negative mining
‚îÇ   ‚îú‚îÄ‚îÄ train_scene_context.py        # Scene context training
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_phase1_4.py          # Phase 1.4 evaluation
‚îÇ
‚îú‚îÄ‚îÄ src/workzone/                     # Core package
‚îÇ   ‚îú‚îÄ‚îÄ detection/                    # Detection components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yolo_detector.py          # YOLO wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cue_classifier.py         # Multi-cue classification
‚îÇ   ‚îú‚îÄ‚îÄ fusion/                       # Multi-modal fusion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_verifier.py          # CLIP semantic verification
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multi_cue_gate.py         # Phase 1.1 AND gate
‚îÇ   ‚îú‚îÄ‚îÄ ocr/                          # OCR text extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_detector.py          # EasyOCR/Paddle wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_classifier.py        # Text category classification
‚îÇ   ‚îú‚îÄ‚îÄ models/                       # Advanced models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scene_context.py          # Phase 1.4 scene classifier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ per_cue_verification.py   # Phase 2.1 per-cue CLIP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trajectory_tracking.py    # Phase 2.1 motion plausibility
‚îÇ   ‚îú‚îÄ‚îÄ temporal/                     # Temporal logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ persistence_tracker.py    # Phase 1.1 persistence
‚îÇ   ‚îú‚îÄ‚îÄ state/                        # State machine
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ workzone_states.py        # State transitions
‚îÇ   ‚îî‚îÄ‚îÄ apps/                         # Applications
‚îÇ       ‚îî‚îÄ‚îÄ streamlit/                # Streamlit UI
‚îÇ           ‚îî‚îÄ‚îÄ app_phase2_1_evaluation.py  # Calibration app
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                        # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_workzone_yolo_setup.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_workzone_yolo_train_eval.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_workzone_yolo_video_demo.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_workzone_video_state_machine.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 05_workzone_video_timeline_calibration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 06_triggered_vlm_semantic_verification.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 07_phase1_4_scene_context.ipynb
‚îÇ
‚îú‚îÄ‚îÄ tests/                            # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_config.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îî‚îÄ‚îÄ test_pipelines.py
‚îÇ
‚îú‚îÄ‚îÄ docs/                             # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ MODEL_REGISTRY.md             # Model performance metrics
‚îÇ   ‚îú‚îÄ‚îÄ PHASE1_3.md                   # Phase 1.3 motion validation
‚îÇ   ‚îî‚îÄ‚îÄ guides/                       # User guides
‚îÇ
‚îî‚îÄ‚îÄ outputs/                          # Processing outputs (gitignored)
    ‚îú‚îÄ‚îÄ phase1_1_demo/
    ‚îú‚îÄ‚îÄ phase1_4_demo/
    ‚îú‚îÄ‚îÄ phase2_1_demo/
    ‚îî‚îÄ‚îÄ ...
```

---

## üî¨ Advanced Usage

### Training Custom Models

#### YOLO Fine-tuning

```bash
# Fine-tune YOLO12 on workzone dataset
python -m workzone.cli.train_yolo \
  --model yolo12s.pt \
  --data data/05_workzone_yolo/workzone_yolo.yaml \
  --imgsz 1280 \
  --batch 32 \
  --epochs 50 \
  --device 0 \
  --run-name yolo12_fine_tuning \
  --project workzone_yolo_fine_tuning
```

#### YOLO11 Training (Latest Architecture)

```bash
# Train with YOLO11 at 1080px resolution using 2 GPUs
python -m workzone.cli.train_yolo \
  --model yolo11.yaml \
  --data data/05_workzone_yolo/workzone_yolo.yaml \
  --imgsz 1080 \
  --batch 96 \
  --epochs 100 \
  --device 0,1 \
  --run-name yolo11_1080px_training \
  --project workzone_yolo11_training
```

#### Scene Context Training

```bash
python scripts/train_scene_context.py \
  --data-root data/05_workzone_yolo \
  --output-dir runs/scene_context \
  --epochs 30 \
  --batch-size 32 \
  --backbone resnet18
```

#### Phase 2.1 Temporal Attention Training

```bash
python scripts/train_phase2_1_attention.py \
  --data-path data/phase2_1_trajectories \
  --output-dir runs/phase2_1_attention \
  --epochs 50 \
  --batch-size 16 \
  --device cuda
```

### Hard-Negative Mining

See [docs/reports/PHASE1_2_MINING_REPORT.md](docs/reports/PHASE1_2_MINING_REPORT.md) for details.

```bash
# 1. Mine candidates from video dataset
bash scripts/HARDNEG_QUICKSTART.sh

# 2. Review and categorize
python scripts/review_hard_negatives.py

# 3. Consolidate annotations
python scripts/consolidate_candidates.py

# 4. Retrain YOLO
python -m workzone.cli.train_yolo \
  --model yolo12s.pt \
  --data data/05_workzone_yolo/workzone_yolo.yaml \
  --imgsz 1280 \
  --device 0,1 \
  --batch 32 \
  --epochs 100 \
  --run-name yolo12_hardneg_training \
  --project workzone_yolo_training
```

---

## üß™ Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=workzone --cov-report=html

# Run specific test
pytest tests/test_models.py -v
```

---

## üìñ Documentation

| Document | Description |
|----------|-------------|
| [docs/guides/APP_TESTING_GUIDE.md](docs/guides/APP_TESTING_GUIDE.md) | **Comprehensive calibration guide** with all parameters explained |
| [docs/JETSON_OPTIMIZATION.md](docs/JETSON_OPTIMIZATION.md) | Edge deployment for Jetson Orin |
| [docs/technical/MODEL_REGISTRY.md](docs/technical/MODEL_REGISTRY.md) | Model performance benchmarks |
| [docs/technical/STREAMLIT_COMPONENT_ANALYSIS.md](docs/technical/STREAMLIT_COMPONENT_ANALYSIS.md) | Component status & architecture |
| [docs/REPOSITORY_STRUCTURE.md](docs/REPOSITORY_STRUCTURE.md) | Detailed folder explanation |



---

## üìÑ License

MIT License - see [LICENSE](alpamayo/LICENSE) for details.

---

## üôè Acknowledgments

- ESV Competition organizers
- Ultralytics for YOLOv12
- OpenAI for CLIP
- PaddleOCR and EasyOCR teams
- W&B for experiment tracking

---

## üìß Contact

For questions or feedback:
- **GitHub Issues**: [github.com/WMaia9/workzone/issues](https://github.com/WMaia9/workzone/issues)

---

**Built with ‚ù§Ô∏è for safer roads**
